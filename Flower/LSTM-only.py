"""
æ¶ˆèå®éªŒï¼šçº¯LSTMæ¨¡å‹ï¼ˆæ— LLMï¼‰
ç”¨äºå¯¹æ¯”LSTM-LLMæ··åˆæ¨¡å‹çš„æ•ˆæœæå‡
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
import logging
import os
import time
from datetime import datetime
warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½®ï¼ˆå¯è°ƒæ•´ï¼‰ ====================
# æ—¶åºçª—å£è®¾ç½®
LOOK_BACK = 48          # å†å²æ—¶é—´æ­¥æ•°ï¼ˆè¾“å…¥ï¼‰
N_FUTURE = 36            # é¢„æµ‹æœªæ¥æ­¥æ•°ï¼ˆè¾“å‡ºï¼‰

# æ•°æ®åˆ’åˆ†æ¯”ä¾‹
TRAIN_RATIO = 0.7       # è®­ç»ƒé›†æ¯”ä¾‹
VAL_RATIO = 0.1         # éªŒè¯é›†æ¯”ä¾‹
TEST_RATIO = 0.2        # æµ‹è¯•é›†æ¯”ä¾‹

# LSTMæ¨¡å‹ç»“æ„ï¼ˆæ›´æ·±æ›´å®½ä»¥åŒ¹é…æ··åˆæ¨¡å‹å®¹é‡ï¼‰
LSTM_HIDDEN_SIZE = 256  # LSTMéšè—å±‚ç»´åº¦
LSTM_NUM_LAYERS = 3     # LSTMå±‚æ•°
LSTM_DROPOUT = 0.3      # LSTM Dropoutç‡

# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = 32         # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 0.001   # åˆå§‹å­¦ä¹ ç‡
WEIGHT_DECAY = 5e-5     # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
EPOCHS = 150            # æœ€å¤§è®­ç»ƒè½®æ•°
EARLY_STOP_PATIENCE = 30  # æ—©åœè€å¿ƒå€¼ï¼ˆéªŒè¯é›†lossä¸é™çš„è½®æ•°ï¼‰
GRAD_CLIP_NORM = 1.0    # æ¢¯åº¦è£å‰ªé˜ˆå€¼

# å…¶ä»–é…ç½®
RANDOM_SEED = 42        # éšæœºç§å­
DATA_FILE = "data/æ•°æ®åˆ—è¡¨ï¼ˆ20240317~20240505ï¼‰.xlsx"  # æ•°æ®æ–‡ä»¶è·¯å¾„
MODEL_SAVE_PATH = "checkpoints/best_lstm_only_model.pth"  # æ¨¡å‹ä¿å­˜è·¯å¾„
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)
log_filename = f"logs/lstm_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è®¾ç½®éšæœºç§å­
def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(RANDOM_SEED)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_filename}")
logger.info("\n" + "="*60)
logger.info("âš™ï¸  è¶…å‚æ•°é…ç½®ï¼ˆçº¯LSTMæ¶ˆèå®éªŒï¼‰")
logger.info("="*60)
logger.info(f"å†å²çª—å£: {LOOK_BACK}æ­¥ ({LOOK_BACK*20}åˆ†é’Ÿ) | é¢„æµ‹çª—å£: {N_FUTURE}æ­¥ ({N_FUTURE*20}åˆ†é’Ÿ)")
logger.info(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒ{TRAIN_RATIO*100}% | éªŒè¯{VAL_RATIO*100}% | æµ‹è¯•{TEST_RATIO*100}%")
logger.info(f"LSTMç»“æ„: {LSTM_NUM_LAYERS}å±‚ Ã— {LSTM_HIDDEN_SIZE}ç»´ (Dropout={LSTM_DROPOUT})")
logger.info(f"è®­ç»ƒå‚æ•°: Batch={BATCH_SIZE} | LR={LEARNING_RATE} | Epochs={EPOCHS} | æ—©åœ={EARLY_STOP_PATIENCE}")

# ==================== 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ====================
logger.info("="*60)
logger.info("1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
logger.info("="*60)

df = pd.read_excel(DATA_FILE)
df = df.iloc[::-1].reset_index(drop=True)
df = df.loc[:,['åœŸå£¤æ¸©åº¦','ç©ºæ°”æ¸©åº¦','ç©ºæ°”æ¹¿åº¦']]

for i in range(len(df)):
    df.iloc[i,0] = float(df.iloc[i,0][:-1])
    df.iloc[i,1] = float(df.iloc[i,1][:-1])
    df.iloc[i,2] = float(df.iloc[i,2][:-1])

logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
logger.info(f"æ•°æ®ç»Ÿè®¡:\n{df.describe()}")

if df.isnull().sum().sum() > 0:
    logger.info(f"âš ï¸ å‘ç°ç¼ºå¤±å€¼: {df.isnull().sum()}")
    df = df.fillna(method='ffill').fillna(method='bfill')
    logger.info("å·²ä½¿ç”¨å‰å‘/åå‘å¡«å……å¤„ç†ç¼ºå¤±å€¼")

# æ•°æ®å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df.values)
logger.info(f"å½’ä¸€åŒ–åæ•°æ®èŒƒå›´: [{scaled_data.min():.3f}, {scaled_data.max():.3f}]")

# ==================== 2. æ—¶åºæ•°æ®æ„å»º ====================
logger.info("\n" + "="*60)
logger.info("2. æ—¶åºæ•°æ®æ„å»º")
logger.info("="*60)

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = pd.DataFrame(data)
    cols, names = list(), list()
    
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [f'var{j+1}(t-{i})' for j in range(n_vars)]
    
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [f'var{j+1}(t)' for j in range(n_vars)]
        else:
            names += [f'var{j+1}(t+{i})' for j in range(n_vars)]
    
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    if dropnan:
        agg.dropna(inplace=True)
    return agg

look_back = LOOK_BACK
n_future = N_FUTURE

supervised_data = series_to_supervised(scaled_data, n_in=look_back, n_out=n_future)
supervised_data = supervised_data.reset_index(drop=True)

input_cols = [col for col in supervised_data.columns if '(t-' in col or col == 'var1(t)']
output_cols = [col for col in supervised_data.columns if 'var1(t+' in col or col == 'var1(t)']

X = supervised_data[input_cols].values
y = supervised_data[output_cols].values

logger.info(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶ X: {X.shape}")
logger.info(f"è¾“å‡ºæ ‡ç­¾å½¢çŠ¶ y: {y.shape}")

# ==================== 3. æ•°æ®é›†åˆ’åˆ† ====================
logger.info("\n" + "="*60)
logger.info("3. æ•°æ®é›†åˆ’åˆ†")
logger.info("="*60)

train_size = int(len(X) * TRAIN_RATIO)
val_size = int(len(X) * VAL_RATIO)

X_train = X[:train_size]
y_train = y[:train_size]

X_val = X[train_size:train_size+val_size]
y_val = y[train_size:train_size+val_size]

X_test = X[train_size+val_size:]
y_test = y[train_size+val_size:]

logger.info(f"è®­ç»ƒé›†: X_train {X_train.shape}, y_train {y_train.shape}")
logger.info(f"éªŒè¯é›†: X_val {X_val.shape}, y_val {y_val.shape}")
logger.info(f"æµ‹è¯•é›†: X_test {X_test.shape}, y_test {y_test.shape}")

# ==================== 4. çº¯LSTMæ¨¡å‹æ¶æ„ ====================
logger.info("\n" + "="*60)
logger.info("4. çº¯LSTMæ¨¡å‹æ¶æ„")
logger.info("="*60)

class PureLSTMModel(nn.Module):
    """çº¯LSTMæ¨¡å‹ï¼ˆæ— LLMç»„ä»¶ï¼‰"""
    def __init__(self, input_size, hidden_size, num_layers, output_steps, dropout=0.2):
        super(PureLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # é¢„æµ‹å¤´ï¼ˆæ›´æ·±çš„ç½‘ç»œä»¥å¼¥è¡¥æ— LLMçš„æŸå¤±ï¼‰
        self.predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2),
            nn.LeakyReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.LeakyReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, output_steps)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x_seq):
        """
        x_seq: (batch, 31) - éœ€è¦reshapeä¸º (batch, 11, 3)
        """
        batch_size = x_seq.shape[0]
        
        # Reshapeä¸ºLSTMè¾“å…¥æ ¼å¼
        x_reshaped = []
        for i in range(batch_size):
            sample_steps = []
            for step in range(10):
                start_idx = step * 3
                sample_steps.append(x_seq[i, start_idx:start_idx+3].unsqueeze(0))
            # æœ€åä¸€æ­¥ï¼šåœŸå£¤æ¸©åº¦ + è¡¥0
            last_step = torch.cat([
                x_seq[i, -1:],
                torch.zeros(2, device=x_seq.device)
            ]).unsqueeze(0)
            sample_steps.append(last_step)
            x_reshaped.append(torch.cat(sample_steps, dim=0))
        
        x_reshaped = torch.stack(x_reshaped)  # (batch, 11, 3)
        
        # LSTMç¼–ç 
        lstm_out, (h_n, c_n) = self.lstm(x_reshaped)
        
        # å–æœ€åä¸€å±‚çš„éšçŠ¶æ€
        features = h_n[-1]  # (batch, hidden_size)
        features = self.dropout(features)
        
        # å¤šæ­¥é¢„æµ‹
        predictions = self.predictor(features)  # (batch, output_steps)
        
        return predictions

# åˆå§‹åŒ–æ¨¡å‹
model = PureLSTMModel(
    input_size=3,
    hidden_size=LSTM_HIDDEN_SIZE,
    num_layers=LSTM_NUM_LAYERS,
    output_steps=n_future,
    dropout=LSTM_DROPOUT
).to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
logger.info(f"æ€»å‚æ•°é‡: {total_params:,}")
logger.info(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")

# ==================== 5. è®­ç»ƒé…ç½® ====================
logger.info("\n" + "="*60)
logger.info("5. è®­ç»ƒé…ç½®")
logger.info("="*60)

# è½¬æ¢ä¸ºTensor
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.FloatTensor(y_train)
X_val_tensor = torch.FloatTensor(X_val)
y_val_tensor = torch.FloatTensor(y_val)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.FloatTensor(y_test)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10
)
criterion = nn.MSELoss()

logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
logger.info(f"è®­ç»ƒè½®æ•°: {EPOCHS}")
logger.info(f"å­¦ä¹ ç‡: {LEARNING_RATE}")

# ==================== 6. è®­ç»ƒå¾ªç¯ ====================
logger.info("\n" + "="*60)
logger.info("6. å¼€å§‹è®­ç»ƒï¼ˆçº¯LSTMï¼‰")
logger.info("="*60)

os.makedirs('checkpoints', exist_ok=True)
model_save_path = MODEL_SAVE_PATH

train_losses = []
val_losses = []
best_val_loss = float('inf')
patience_counter = 0
early_stop_patience = 20

for epoch in range(EPOCHS):
    epoch_start_time = time.time()
    
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0.0
    train_start_time = time.time()
    
    for batch_x, batch_y in train_loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
        optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    train_losses.append(train_loss)
    train_time = time.time() - train_start_time
    
    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0.0
    val_start_time = time.time()
    
    with torch.no_grad():
        for batch_x, batch_y in val_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            val_loss += loss.item()
    
    val_loss /= len(val_loader)
    val_losses.append(val_loss)
    val_time = time.time() - val_start_time
    epoch_time = time.time() - epoch_start_time
    
    scheduler.step(val_loss)
    
    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), model_save_path)
        save_status = "âœ“ å·²ä¿å­˜"
        patience_counter = 0
    else:
        save_status = f"æœªä¿å­˜ (patience: {patience_counter + 1}/{EARLY_STOP_PATIENCE})"
        patience_counter += 1
    
    logger.info(
        f"Epoch [{epoch+1:3d}/{EPOCHS}] "
        f"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | "
        f"LR: {optimizer.param_groups[0]['lr']:.6f} | "
        f"Time: {epoch_time:.2f}s (Train: {train_time:.2f}s, Val: {val_time:.2f}s) | "
        f"{save_status}"
    )
    
    if patience_counter >= EARLY_STOP_PATIENCE:
        logger.info(f"Early stopping at epoch {epoch+1}")
        break

# ==================== 7. æµ‹è¯•è¯„ä¼° ====================
logger.info("\n" + "="*60)
logger.info("7. æµ‹è¯•é›†è¯„ä¼°")
logger.info("="*60)

model.load_state_dict(torch.load(model_save_path))
logger.info(f"âœ“ å·²åŠ è½½æœ€ä½³æ¨¡å‹: {model_save_path}")
model.eval()

all_predictions = []
all_targets = []

with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        outputs = model(batch_x)
        
        all_predictions.append(outputs.cpu().numpy())
        all_targets.append(batch_y.cpu().numpy())

predictions = np.vstack(all_predictions)
targets = np.vstack(all_targets)

# åå½’ä¸€åŒ–åˆ°åŸå§‹å°ºåº¦
predictions_real = np.zeros_like(predictions)
targets_real = np.zeros_like(targets)

for step in range(n_future):
    step_pred = np.hstack([
        predictions[:, step:step+1],
        np.zeros((predictions.shape[0], 2))
    ])
    step_target = np.hstack([
        targets[:, step:step+1],
        np.zeros((targets.shape[0], 2))
    ])
    
    predictions_real[:, step] = scaler.inverse_transform(step_pred)[:, 0]
    targets_real[:, step] = scaler.inverse_transform(step_target)[:, 0]

# è®¡ç®—æŒ‡æ ‡
mse = mean_squared_error(targets_real, predictions_real)
rmse = np.sqrt(mse)
mae = mean_absolute_error(targets_real, predictions_real)
r2 = r2_score(targets_real.flatten(), predictions_real.flatten())

logger.info(f"æµ‹è¯•é›†ç»“æœï¼ˆçº¯LSTMï¼‰:")
logger.info(f"  MSE:  {mse:.4f}")
logger.info(f"  RMSE: {rmse:.4f}")
logger.info(f"  MAE:  {mae:.4f}")
logger.info(f"  RÂ²:   {r2:.4f}")

# ==================== 8. ä¿å­˜è®­ç»ƒæ—¥å¿— ====================
logger.info("\n" + "="*60)
logger.info("8. ä¿å­˜è®­ç»ƒæ—¥å¿—")
logger.info("="*60)

import json

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_json_filename = f"logs/lstm_only_{timestamp}.json"

log_data = {
    'experiment_type': 'LSTM-only (Ablation Study)',
    'timestamp': timestamp,
    'hyperparameters': {
        'look_back': look_back,
        'n_future': n_future,
        'batch_size': BATCH_SIZE,
        'epochs': len(train_losses),
        'learning_rate': LEARNING_RATE,
        'weight_decay': WEIGHT_DECAY,
        'lstm_hidden_size': 256,
        'lstm_num_layers': 3,
        'dropout': 0.2
    },
    'data_split': {
        'train_samples': len(X_train),
        'val_samples': len(X_val),
        'test_samples': len(X_test),
        'train_ratio': 0.7,
        'val_ratio': 0.1,
        'test_ratio': 0.2
    },
    'training_history': {
        'train_losses': [float(x) for x in train_losses],
        'val_losses': [float(x) for x in val_losses],
        'best_epoch': val_losses.index(min(val_losses)) + 1,
        'best_train_loss': float(min(train_losses)),
        'best_val_loss': float(min(val_losses))
    },
    'test_results': {
        'mse': float(mse),
        'rmse': float(rmse),
        'mae': float(mae),
        'r2': float(r2)
    },
    'model_info': {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'model_path': model_save_path
    }
}

with open(log_json_filename, 'w', encoding='utf-8') as f:
    json.dump(log_data, f, indent=2, ensure_ascii=False)

logger.info(f"è®­ç»ƒæ—¥å¿—å·²ä¿å­˜è‡³: {log_json_filename}")

# ==================== 9. è¾“å‡ºæœ€ç»ˆç»“æœ ====================
logger.info("\n" + "="*60)
logger.info("è®­ç»ƒå®Œæˆï¼æœ€ç»ˆç»“æœï¼ˆçº¯LSTMï¼‰")
logger.info("="*60)
logger.info(f"\nã€æ¨¡å‹ä¿¡æ¯ã€‘")
logger.info(f"  æ¨¡å‹ç±»å‹: çº¯LSTMï¼ˆæ¶ˆèå®éªŒï¼‰")
logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
logger.info(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
logger.info(f"  æœ€ä½³æ¨¡å‹: {model_save_path}")

logger.info(f"\nã€è®­ç»ƒä¿¡æ¯ã€‘")
logger.info(f"  è®­ç»ƒè½®æ•°: {len(train_losses)} epochs")
logger.info(f"  æœ€ä½³è®­ç»ƒLoss: {min(train_losses):.6f} (Epoch {train_losses.index(min(train_losses))+1})")
logger.info(f"  æœ€ä½³éªŒè¯Loss: {min(val_losses):.6f} (Epoch {val_losses.index(min(val_losses))+1})")

logger.info(f"\nã€æµ‹è¯•é›†ç»“æœã€‘")
logger.info(f"  MSE:  {mse:.6f}")
logger.info(f"  RMSE: {rmse:.6f}â„ƒ")
logger.info(f"  MAE:  {mae:.6f}â„ƒ")
logger.info(f"  RÂ²:   {r2:.6f}")

logger.info(f"\nã€å„æ­¥é¢„æµ‹MAEã€‘")
for step in range(n_future):
    step_mae = mean_absolute_error(targets_real[:, step], predictions_real[:, step])
    logger.info(f"  Step {step+1}: {step_mae:.6f}â„ƒ")

logger.info("\n" + "="*60)
logger.info("ğŸ’¡ æç¤ºï¼šä¸ LSTM-LLM æ··åˆæ¨¡å‹å¯¹æ¯”ä»¥è¯„ä¼°LLMç»„ä»¶çš„è´¡çŒ®")
logger.info("="*60)
