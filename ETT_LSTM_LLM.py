"""
ETDatasetç”µåŠ›è´Ÿè·é¢„æµ‹ï¼šLSTM-LLMæ··åˆæ¨¡å‹ï¼ˆProLLMæ¶æ„ç‰ˆæœ¬ï¼‰
æ ¸å¿ƒæ”¹è¿›ï¼šå°†èåˆç‰¹å¾é€šè¿‡GPT-2 forwardä¼ æ’­ï¼Œåˆ©ç”¨Transformerè¿›è¡Œè¯­ä¹‰æ¨ç†
å‚è€ƒProLLMæ¶æ„ï¼Œä½¿ç”¨LLMçš„åŠ¨æ€å¤„ç†èƒ½åŠ›è€Œéé™æ€embeddings
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from transformers import GPT2Model, GPT2Config
import warnings
import logging
import os
import json
from datetime import datetime
warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
LOOK_BACK = 96          # å†å²çª—å£ï¼š96å°æ—¶ï¼ˆ4å¤©ï¼‰
N_FUTURE = 24           # é¢„æµ‹çª—å£ï¼š24å°æ—¶ï¼ˆ1å¤©ï¼‰
TRAIN_RATIO = 0.7       # æ”¹ä¸º70/30åˆ’åˆ†
TEST_RATIO = 0.3

LSTM_HIDDEN_SIZE = 128
LSTM_NUM_LAYERS = 2
LSTM_DROPOUT = 0.4      # å¢å¼ºæ­£åˆ™åŒ–ï¼šä»0.2å¢åŠ åˆ°0.4

BATCH_SIZE = 32
LEARNING_RATE = 0.0003  # å¤§å¹…é™ä½å­¦ä¹ ç‡ï¼šä»0.001é™è‡³0.0003
WEIGHT_DECAY = 5e-4     # å¢å¼ºL2æ­£åˆ™ï¼šä»1e-5å¢åŠ åˆ°5e-4
EPOCHS = 50             # å‡å°‘epochsé˜²æ­¢è¿‡æ‹Ÿåˆ
EARLY_STOP_PATIENCE = 8 # æ›´æ¿€è¿›çš„æ—©åœï¼šä»15ç¼©çŸ­åˆ°8
GRAD_CLIP_NORM = 1.0

RANDOM_SEED = 42
DATA_FILE = "ETDataset/ETT-small/ETTh1.csv"
EMBEDDING_DIR = "embeddings/ett"  # é¢„ç”Ÿæˆçš„embeddingsç›®å½•
MODEL_SAVE_PATH = "checkpoints/best_ett_lstm_llm.pth"
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

# é…ç½®æ—¥å¿—
os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)
os.makedirs('results', exist_ok=True)
log_filename = f"logs/ett_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(RANDOM_SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

logger.info("="*60)
logger.info("âš¡ ETDatasetç”µåŠ›è´Ÿè·é¢„æµ‹ - LSTM-LLMæ··åˆæ¨¡å‹ï¼ˆProLLMæ¶æ„ï¼‰")
logger.info("="*60)
logger.info(f"è®¾å¤‡: {device}")
logger.info(f"æ—¥å¿—: {log_filename}")
logger.info(f"å†å²çª—å£: {LOOK_BACK}å°æ—¶ | é¢„æµ‹çª—å£: {N_FUTURE}å°æ—¶")
logger.info(f"LSTMç»“æ„: {LSTM_NUM_LAYERS}å±‚ Ã— {LSTM_HIDDEN_SIZE}ç»´")

# ==================== 1. æ•°æ®åŠ è½½ ====================
logger.info("\n" + "="*60)
logger.info("1. åŠ è½½ETDatasetæ•°æ®")
logger.info("="*60)

df = pd.read_csv(DATA_FILE)
df['date'] = pd.to_datetime(df['date'])
logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]}æ¡è®°å½•")

# æå–ç‰¹å¾ï¼ˆå»é™¤æ—¥æœŸåˆ—ï¼‰
feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']
target_col = 'OT'  # æ²¹æ¸©ä½œä¸ºé¢„æµ‹ç›®æ ‡

data = df[feature_cols + [target_col]].values

# åŠ è½½é¢„ä¿å­˜çš„scaler
scaler_mean = np.load(f"{EMBEDDING_DIR}/scaler_mean.npy")
scaler_scale = np.load(f"{EMBEDDING_DIR}/scaler_scale.npy")
scaler = StandardScaler()
scaler.mean_ = scaler_mean
scaler.scale_ = scaler_scale

data_scaled = scaler.transform(data)

logger.info(f"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨é¢„ä¿å­˜çš„scalerï¼‰")

# ==================== 2. æ„å»ºæ—¶åºæ•°æ®é›† ====================
def create_sequences(data, look_back, n_future):
    X, y = [], []
    for i in range(len(data) - look_back - n_future + 1):
        X.append(data[i:i+look_back, :])
        y.append(data[i+look_back:i+look_back+n_future, -1])  # åªé¢„æµ‹OT
    return np.array(X), np.array(y)

X, y = create_sequences(data_scaled, LOOK_BACK, N_FUTURE)
logger.info(f"\nâœ“ æ—¶åºæ•°æ®æ„å»ºå®Œæˆ")
logger.info(f"  X shape: {X.shape} (æ ·æœ¬æ•°, æ—¶é—´æ­¥, ç‰¹å¾æ•°)")
logger.info(f"  y shape: {y.shape} (æ ·æœ¬æ•°, é¢„æµ‹æ­¥æ•°)")

# æ•°æ®åˆ’åˆ†ï¼ˆ70/30ï¼‰
train_size = int(len(X) * TRAIN_RATIO)

X_train = X[:train_size]
y_train = y[:train_size]
X_test = X[train_size:]
y_test = y[train_size:]

logger.info(f"âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
logger.info(f"  è®­ç»ƒé›†: {len(X_train)}æ ·æœ¬ ({TRAIN_RATIO*100:.0f}%)")
logger.info(f"  æµ‹è¯•é›†: {len(X_test)}æ ·æœ¬ ({TEST_RATIO*100:.0f}%)")

# ==================== 3. åŠ è½½é¢„ç”Ÿæˆçš„Embeddings ====================
logger.info("\n" + "="*60)
logger.info("3. åŠ è½½é¢„ç”Ÿæˆçš„GPT-2 Prompt Embeddings")
logger.info("="*60)

def load_embeddings(save_dir):
    """åŠ è½½åˆ†å—ä¿å­˜çš„embeddings"""
    embeddings = []
    i = 0
    while os.path.exists(f"{save_dir}/embeddings_{i}.npy"):
        chunk = np.load(f"{save_dir}/embeddings_{i}.npy")
        embeddings.append(chunk)
        i += 1
    if len(embeddings) == 0:
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°embeddingsæ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ: python generate_embeddings_ett.py"
        )
    return np.vstack(embeddings)

train_embeddings = load_embeddings(f"{EMBEDDING_DIR}/train")
test_embeddings = load_embeddings(f"{EMBEDDING_DIR}/test")

logger.info(f"âœ“ EmbeddingsåŠ è½½å®Œæˆ")
logger.info(f"  è®­ç»ƒé›†: {train_embeddings.shape}")
logger.info(f"  æµ‹è¯•é›†: {test_embeddings.shape}")

# éªŒè¯å°ºå¯¸åŒ¹é…
assert len(train_embeddings) == len(X_train), "è®­ç»ƒé›†å°ºå¯¸ä¸åŒ¹é…ï¼"
assert len(test_embeddings) == len(X_test), "æµ‹è¯•é›†å°ºå¯¸ä¸åŒ¹é…ï¼"
logger.info(f"âœ“ æ•°æ®å°ºå¯¸éªŒè¯é€šè¿‡")

# ==================== 4. æ•°æ®é›†ç±» ====================
class ETTDataset(Dataset):
    def __init__(self, X, y, embeddings):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.embeddings = torch.FloatTensor(embeddings)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.embeddings[idx], self.y[idx]

train_dataset = ETTDataset(X_train, y_train, train_embeddings)
test_dataset = ETTDataset(X_test, y_test, test_embeddings)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ==================== 5. æ¨¡å‹å®šä¹‰ï¼ˆProLLMæ¶æ„ï¼‰ ====================
class LSTMLLM_ETT(nn.Module):
    def __init__(self, lstm_input_size, lstm_hidden_size, llm_hidden_size, 
                 output_steps, num_layers=2, dropout=0.2):
        super(LSTMLLM_ETT, self).__init__()
        
        # LSTMåˆ†æ”¯ï¼šç¼–ç æ—¶åºæ¨¡å¼
        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size, 
                           num_layers=num_layers, dropout=dropout, 
                           batch_first=True)
        
        # LLM prompt embeddingsæŠ•å½±ï¼š768ç»´ â†’ 128ç»´
        self.llm_prompt_projector = nn.Sequential(
            nn.Linear(llm_hidden_size, lstm_hidden_size),
            nn.LayerNorm(lstm_hidden_size),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # é—¨æ§èåˆå•å…ƒï¼šå†³å®šLSTMä¸LLM promptçš„èåˆæ¯”ä¾‹
        self.fusion_gate = nn.Sequential(
            nn.Linear(lstm_hidden_size * 2, lstm_hidden_size),
            nn.Tanh(),
            nn.Linear(lstm_hidden_size, lstm_hidden_size),
            nn.Sigmoid()
        )
        
        # ã€æ ¸å¿ƒæ”¹è¿›ã€‘åå‘æŠ•å½±å±‚ï¼š128ç»´ â†’ 768ç»´ï¼ˆå‡†å¤‡è¾“å…¥GPT-2ï¼‰
        self.to_llm_projector = nn.Sequential(
            nn.Linear(lstm_hidden_size, llm_hidden_size),
            nn.LayerNorm(llm_hidden_size),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # ã€æ ¸å¿ƒæ”¹è¿›ã€‘åŠ è½½é¢„è®­ç»ƒGPT-2æ¨¡å‹ï¼ˆå†»ç»“å‚æ•°ï¼‰
        logger.info("  æ­£åœ¨åŠ è½½GPT-2æ¨¡å‹...")
        config = GPT2Config.from_pretrained('gpt2')
        self.gpt2 = GPT2Model.from_pretrained('gpt2', config=config)
        # å†»ç»“GPT-2å‚æ•°ï¼Œåªè®­ç»ƒæŠ•å½±å±‚å’Œèåˆå±‚
        for param in self.gpt2.parameters():
            param.requires_grad = False
        logger.info("  âœ“ GPT-2åŠ è½½å®Œæˆï¼ˆå‚æ•°å·²å†»ç»“ï¼‰")
        
        # é¢„æµ‹å¤´ï¼šä»LLMè¾“å‡ºåˆ°æœ€ç»ˆé¢„æµ‹
        self.predictor = nn.Sequential(
            nn.Linear(llm_hidden_size, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(128, output_steps)
        )
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼Œé¿å…åå‘æŸä¸€åˆ†æ”¯"""
        for name, m in self.named_modules():
            # è·³è¿‡GPT-2çš„å‚æ•°ï¼ˆå·²é¢„è®­ç»ƒï¼‰
            if 'gpt2' in name:
                continue
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x_lstm, x_llm_prompt):
        """
        ProLLMæ¶æ„çš„forwardæµç¨‹ï¼š
        1. LSTMç¼–ç æ—¶åºç‰¹å¾
        2. LLM prompté™ç»´å¹¶é€šè¿‡é—¨æ§èåˆ
        3. ã€å…³é”®ã€‘èåˆç‰¹å¾æŠ•å½±åˆ°LLMç©ºé—´ï¼Œé€šè¿‡GPT-2 forward
        4. GPT-2è¾“å‡ºç”¨äºæœ€ç»ˆé¢„æµ‹
        
        Args:
            x_lstm: (batch, seq_len, input_size) æ—¶åºè¾“å…¥
            x_llm_prompt: (batch, 768) é¢„ç”Ÿæˆçš„prompt embeddings
        """
        batch_size = x_lstm.size(0)
        
        # 1. LSTMç¼–ç æ—¶åºç‰¹å¾
        lstm_out, _ = self.lstm(x_lstm)  # (batch, seq_len, 128)
        lstm_feat = lstm_out[:, -1, :]   # (batch, 128)
        
        # 2. LLM promptæŠ•å½±åˆ°ç›¸åŒç»´åº¦
        llm_prompt_feat = self.llm_prompt_projector(x_llm_prompt)  # (batch, 128)
        
        # 3. ç‰¹å¾çº§é—¨æ§èåˆ
        combined = torch.cat([lstm_feat, llm_prompt_feat], dim=1)  # (batch, 256)
        gate = self.fusion_gate(combined)  # (batch, 128)
        
        # æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼ˆå‚è€ƒProLLMï¼‰
        gate = torch.clamp(gate, min=0.0, max=1.0)
        gate = torch.nan_to_num(gate, nan=0.5)
        
        # é€ç»´åº¦åŠ æƒèåˆ
        fused = gate * lstm_feat + (1 - gate) * llm_prompt_feat  # (batch, 128)
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        fused = torch.nan_to_num(fused, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 4. ã€æ ¸å¿ƒæ”¹è¿›ã€‘æŠ•å½±åˆ°LLMç»´åº¦å¹¶é€šè¿‡GPT-2 forward
        fused_llm_input = self.to_llm_projector(fused)  # (batch, 768)
        fused_llm_input = fused_llm_input.unsqueeze(1)  # (batch, 1, 768) ä½œä¸ºå•ä¸ªtoken
        
        # é€šè¿‡GPT-2è¿›è¡Œè¯­ä¹‰æ¨ç†ï¼ˆåˆ©ç”¨12å±‚Transformerçš„self-attentionï¼‰
        gpt2_output = self.gpt2(inputs_embeds=fused_llm_input)
        llm_semantic_feat = gpt2_output.last_hidden_state[:, -1, :]  # (batch, 768)
        
        # æ•°å€¼ç¨³å®šæ€§å¤„ç†
        llm_semantic_feat = torch.nan_to_num(llm_semantic_feat, nan=0.0)
        
        # 5. é¢„æµ‹
        output = self.predictor(llm_semantic_feat)  # (batch, output_steps)
        
        # è¿”å›å¹³å‡gateæƒé‡ä½œä¸ºå¯è§£é‡Šæ€§æŒ‡æ ‡
        avg_gate = gate.mean(dim=1, keepdim=True)  # (batch, 1)
        return output, avg_gate

model = LSTMLLM_ETT(
    lstm_input_size=7,
    lstm_hidden_size=LSTM_HIDDEN_SIZE,
    llm_hidden_size=768,
    output_steps=N_FUTURE,
    num_layers=LSTM_NUM_LAYERS,
    dropout=LSTM_DROPOUT
).to(device)

logger.info(f"\nâœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
logger.info(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ==================== 6. è®­ç»ƒ ====================
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
criterion = nn.MSELoss()
# æ·»åŠ å­¦ä¹ ç‡è¡°å‡
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5, verbose=True
)

logger.info("\n" + "="*60)
logger.info("å¼€å§‹è®­ç»ƒï¼ˆ70/30åˆ’åˆ†ï¼Œå¢å¼ºæ­£åˆ™åŒ–ï¼‰")
logger.info("="*60)

best_train_loss = float('inf')
patience_counter = 0
gate_weights_history = []

for epoch in range(EPOCHS):
    # è®­ç»ƒ
    model.train()
    train_loss = 0
    train_preds, train_trues = [], []
    epoch_gate_weights = []
    
    for X_batch, emb_batch, y_batch in train_loader:
        X_batch, emb_batch, y_batch = X_batch.to(device), emb_batch.to(device), y_batch.to(device)
        
        optimizer.zero_grad()
        pred, gate_weight = model(X_batch, emb_batch)
        loss = criterion(pred, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
        optimizer.step()
        
        train_loss += loss.item()
        train_preds.append(pred.detach().cpu().numpy())
        train_trues.append(y_batch.detach().cpu().numpy())
        epoch_gate_weights.append(gate_weight.mean().item())
    
    train_loss /= len(train_loader)
    train_preds = np.vstack(train_preds)
    train_trues = np.vstack(train_trues)
    train_r2 = r2_score(train_trues.flatten(), train_preds.flatten())
    avg_gate_weight = np.mean(epoch_gate_weights)
    gate_weights_history.append(avg_gate_weight)
    
    # å­¦ä¹ ç‡è¡°å‡
    scheduler.step(train_loss)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹å¹¶æ—©åœ
    if train_loss < best_train_loss:
        best_train_loss = train_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        patience_counter = 0
        save_status = "âœ“"
    else:
        patience_counter += 1
        save_status = ""
    
    # æ¯è½®éƒ½æ‰“å°
    logger.info(
        f"Epoch {epoch+1:3d}/{EPOCHS} | "
        f"Loss: {train_loss:.4f} | RÂ²: {train_r2:.4f} | "
        f"Gate: {avg_gate_weight:.3f} | {save_status}"
    )
    
    # æ—©åœæ£€æŸ¥
    if patience_counter >= EARLY_STOP_PATIENCE:
        logger.info(f"\næ—©åœè§¦å‘ï¼è¿ç»­{EARLY_STOP_PATIENCE}è½®æ— æ”¹å–„")
        break

# ==================== 7. æµ‹è¯• ====================
logger.info("\n" + "="*60)
logger.info("7. æµ‹è¯•é›†è¯„ä¼°")
logger.info("="*60)

model.load_state_dict(torch.load(MODEL_SAVE_PATH))
model.eval()

all_preds = []
all_trues = []
test_gate_weights = []

with torch.no_grad():
    for X_batch, emb_batch, y_batch in test_loader:
        X_batch, emb_batch, y_batch = X_batch.to(device), emb_batch.to(device), y_batch.to(device)
        pred, gate_weight = model(X_batch, emb_batch)
        all_preds.append(pred.cpu().numpy())
        all_trues.append(y_batch.cpu().numpy())
        test_gate_weights.append(gate_weight.cpu().numpy())

y_pred = np.vstack(all_preds)
y_true = np.vstack(all_trues)
test_gate_weights = np.vstack(test_gate_weights)

# åæ ‡å‡†åŒ–ï¼ˆåªé’ˆå¯¹OTåˆ—ï¼‰
ot_scaler = StandardScaler()
ot_scaler.mean_ = scaler.mean_[-1]
ot_scaler.scale_ = scaler.scale_[-1]

y_pred_original = ot_scaler.inverse_transform(y_pred)
y_true_original = ot_scaler.inverse_transform(y_true)

# è®¡ç®—æŒ‡æ ‡
mse = mean_squared_error(y_true_original, y_pred_original)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true_original, y_pred_original)
r2 = r2_score(y_true_original.flatten(), y_pred_original.flatten())

logger.info("="*60)
logger.info("ğŸ“Š æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡")
logger.info("="*60)
logger.info(f"  RMSE: {rmse:.4f}Â°C")
logger.info(f"  MAE:  {mae:.4f}Â°C")
logger.info(f"  RÂ²:   {r2:.4f}")
logger.info(f"  å¹³å‡Gateæƒé‡: {test_gate_weights.mean():.3f} (0.5è¡¨ç¤ºLSTMä¸LLMè´¡çŒ®ç›¸å½“)")
logger.info("="*60)

# å¯è§†åŒ–
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

fig, axes = plt.subplots(2, 1, figsize=(14, 8))

# é¢„æµ‹å¯¹æ¯”
axes[0].plot(y_true_original[:200, 0], label='çœŸå®å€¼', alpha=0.7)
axes[0].plot(y_pred_original[:200, 0], label='é¢„æµ‹å€¼', alpha=0.7)
axes[0].set_title(f'ETTç”µåŠ›è´Ÿè·é¢„æµ‹ (LSTM-LLM ProLLMæ¶æ„) | RMSE={rmse:.4f}Â°C, RÂ²={r2:.4f}')
axes[0].set_xlabel('æ—¶é—´æ­¥')
axes[0].set_ylabel('æ²¹æ¸© (Â°C)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Gateæƒé‡åˆ†å¸ƒ
axes[1].hist(test_gate_weights.flatten(), bins=50, alpha=0.7, edgecolor='black')
axes[1].axvline(test_gate_weights.mean(), color='red', linestyle='--', 
                label=f'å‡å€¼={test_gate_weights.mean():.3f}')
axes[1].set_title('ç‰¹å¾çº§Gateæƒé‡åˆ†å¸ƒï¼ˆ0.5è¡¨ç¤ºLSTMä¸LLMè´¡çŒ®ç›¸å½“ï¼‰')
axes[1].set_xlabel('Gateæƒé‡')
axes[1].set_ylabel('é¢‘æ•°')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/ett_lstm_llm_results.png', dpi=150, bbox_inches='tight')
logger.info(f"\nâœ“ ç»“æœå›¾ä¿å­˜è‡³: results/ett_lstm_llm_results.png")

logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
