"""
çº¯LSTMåˆ†ç±»æ¨¡å‹ - ProLLMæ•°æ®é›†å¯¹ç…§ç»„
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
import warnings
import logging
import os
import time
from datetime import datetime

warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
import argparse

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--dataset", type=str, default=None, help="æ•°æ®é›†åç§°ï¼Œä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰con*Sensor")
parser.add_argument("--device", type=str, default="cuda", choices=["cpu", "cuda"], help="è®¡ç®—è®¾å¤‡")
args = parser.parse_args()

DATA_DIR = "ProLLM/con_normalized"

LSTM_HIDDEN_SIZE = 128
LSTM_NUM_LAYERS = 2
LSTM_DROPOUT = 0.2 

BATCH_SIZE = 4  # å¯¹é½ProLLMï¼š16 -> 4
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4 
EPOCHS = 100
GRAD_CLIP_NORM = 1.0

RANDOM_SEED = 42
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(RANDOM_SEED)
device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

# ==================== è·å–æ•°æ®é›†åˆ—è¡¨ ====================
if args.dataset:
    datasets = [args.dataset]
else:
    datasets = [d for d in os.listdir(DATA_DIR) 
               if os.path.isdir(os.path.join(DATA_DIR, d)) and 'Sensor' in d]
    datasets.sort()
    print(f"\nå°†è®­ç»ƒ {len(datasets)} ä¸ªæ•°æ®é›†:")
    for ds in datasets:
        print(f"  - {ds}")
    print()

# ==================== è®­ç»ƒæ¯ä¸ªæ•°æ®é›† ====================
for DATASET_NAME in datasets:
    print("\n" + "="*80)
    print(f"å¼€å§‹è®­ç»ƒæ•°æ®é›†: {DATASET_NAME}")
    print("="*80)
    
    MODEL_SAVE_PATH = f"checkpoints/best_lstm_only_{DATASET_NAME}.pth"
    log_filename = f"logs/{DATASET_NAME}_lstm_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é‡æ–°é…ç½®logger
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    
    logger.info("="*60)
    logger.info(f"âš¡ çº¯LSTMåˆ†ç±»æ¨¡å‹ - {DATASET_NAME}")
    logger.info("="*60)
    
    # æ•°æ®åŠ è½½
    train_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_x.npy"
    train_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_y.npy"
    test_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_x.npy"
    test_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_y.npy"
    
    X_train = np.load(train_x_path)  # (N, channels, length)
    y_train = np.load(train_y_path)
    X_test = np.load(test_x_path)
    y_test = np.load(test_y_path)
    
    # å°†æ ‡ç­¾æ˜ å°„åˆ°0-basedç´¢å¼• (CrossEntropyLossè¦æ±‚)
    y_train = y_train - 1
    y_test = y_test - 1
    
    num_classes = len(np.unique(y_train))
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†: {X_train.shape}")
    logger.info(f"  æµ‹è¯•é›†: {X_test.shape}")
    logger.info(f"  ç±»åˆ«æ•°: {num_classes}")
    
    # Dataset
    class SimpleDataset(Dataset):
        def __init__(self, X, y):
            # X: (N, channels, length) -> (N, length, channels)
            self.X = torch.FloatTensor(X).permute(0, 2, 1)
            self.y = torch.LongTensor(y)
        
        def __len__(self):
            return len(self.X)
        
        def __getitem__(self, idx):
            return self.X[idx], self.y[idx]
    
    train_dataset = SimpleDataset(X_train, y_train)
    test_dataset = SimpleDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # çº¯LSTMæ¨¡å‹
    class PureLSTM_Classification(nn.Module):
        def __init__(self, input_size, hidden_size, num_classes, num_layers=2, dropout=0.3):
            super(PureLSTM_Classification, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, 
                               dropout=dropout, batch_first=True)
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size, 256),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(256, 128),
                nn.GELU(),
                nn.Dropout(dropout * 0.5),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            logits = self.classifier(lstm_out[:, -1, :])
            return logits
    
    model = PureLSTM_Classification(
        input_size=X_train.shape[1],
        hidden_size=LSTM_HIDDEN_SIZE,
        num_classes=num_classes,
        num_layers=LSTM_NUM_LAYERS,
        dropout=LSTM_DROPOUT
    ).to(device)
    
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒ
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss()
    # å¯¹é½ProLLMï¼šä½¿ç”¨StepLRæ›¿ä»£ReduceLROnPlateau
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=50, gamma=0.8
    )
    
    logger.info("\nå¼€å§‹è®­ç»ƒ...")
    best_test_acc = 0
    training_start_time = time.time()
    
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        train_loss = 0
        train_preds, train_trues = [], []
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            logits = model(X_batch)
            loss = criterion(logits, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
            optimizer.step()
            train_loss += loss.item()
            train_preds.extend(logits.argmax(dim=1).cpu().numpy())
            train_trues.extend(y_batch.cpu().numpy())
        
        train_loss /= len(train_loader)
        train_acc = accuracy_score(train_trues, train_preds)
        
        # æµ‹è¯•
        model.eval()
        test_preds, test_trues = [], []
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch = X_batch.to(device)
                logits = model(X_batch)
                test_preds.extend(logits.argmax(dim=1).cpu().numpy())
                test_trues.extend(y_batch.cpu().numpy())
        
        test_acc = accuracy_score(test_trues, test_preds)
        test_f1 = f1_score(test_trues, test_preds, average='weighted')
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            save_status = "âœ“"
        else:
            save_status = ""
        
        # è®¡ç®—epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        
        logger.info(
            f"Epoch {epoch+1:3d}/{EPOCHS} | "
            f"TrLoss: {train_loss:.4f} | TrAcc: {train_acc:.4f} | "
            f"TeAcc: {test_acc:.4f} | F1: {test_f1:.4f} | "
            f"Time: {epoch_time:.2f}s | {save_status}"
        )
        
        # å­¦ä¹ ç‡è¡°å‡ï¼ˆå¯¹é½ProLLMï¼šåœ¨epochç»“æŸæ—¶è°ƒç”¨ï¼‰
        scheduler.step()
    
    # æœ€ç»ˆè¯„ä¼°
    total_training_time = time.time() - training_start_time
    logger.info(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_training_time:.2f}s ({total_training_time/60:.2f}min)")
    
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    model.eval()
    
    test_preds, test_trues = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            logits = model(X_batch)
            test_preds.extend(logits.argmax(dim=1).cpu().numpy())
            test_trues.extend(y_batch.cpu().numpy())
    
    test_acc = accuracy_score(test_trues, test_preds)
    test_f1 = f1_score(test_trues, test_preds, average='weighted')
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æœ€ç»ˆæ€§èƒ½ï¼ˆçº¯LSTMï¼‰")
    logger.info("="*60)
    logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"  æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"\n{classification_report(test_trues, test_preds)}")
    
    print(f"\nâœ… æ•°æ®é›† {DATASET_NAME} è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}\n")

print("\n" + "="*80)
print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
print("="*80)
