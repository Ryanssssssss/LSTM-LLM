"""
LSTM-LLMåˆ†ç±»æ¨¡å‹ - ä½¿ç”¨ProLLMçš„æ•°æ®é›†
ä¿æŒLSTM-LLMæ¶æ„ï¼Œæ”¹ä¸ºåˆ†ç±»ä»»åŠ¡
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
from transformers import RobertaModel, RobertaConfig
import warnings
import logging
import os
import sys
import time
from datetime import datetime

# å¯¼å…¥æ ¹ç›®å½•çš„ PromptHandler
from prompt_handler import PromptHandler

warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
import argparse

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--dataset", type=str, default=None, help="æ•°æ®é›†åç§°ï¼Œä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰con*Sensor")
parser.add_argument("--device", type=str, default="cuda", choices=["cpu", "cuda"], help="è®¡ç®—è®¾å¤‡")
args = parser.parse_args()

DATA_DIR = "ProLLM/con_normalized"

LSTM_HIDDEN_SIZE = 128
LSTM_NUM_LAYERS = 2
LSTM_DROPOUT = 0.1  # å¯¹é½ProLLMï¼š0.3 -> 0.1

# Patchå‚æ•°ï¼ˆå­¦ä¹ ProLLMï¼‰
PATCH_LEN = 16
STRIDE = 8

BATCH_SIZE = 4  # å¯¹é½ProLLMï¼š16 -> 4
LEARNING_RATE = 0.001
LLM_LEARNING_RATE = 0.0001  # RoBERTaå±‚ä¸“ç”¨å­¦ä¹ ç‡
WEIGHT_DECAY = 1e-4  
EPOCHS = 50
GRAD_CLIP_NORM = 1.0

# æ—©åœé…ç½®
EARLY_STOP_PATIENCE = 10  # æµ‹è¯•å‡†ç¡®ç‡è¿ç»­Nä¸ªepochä¸æå‡åˆ™åœæ­¢
EARLY_STOP_THRESHOLD = 0.995  # å‡†ç¡®ç‡è¾¾åˆ°æ­¤é˜ˆå€¼åæ‰å¼€å§‹è®¡æ•°

RANDOM_SEED = 42
USE_OFFLINE_EMBEDDINGS = True
PROMPT_REPRESENTATION = "pooled_last_token"
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(RANDOM_SEED)
device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

# ==================== è·å–æ•°æ®é›†åˆ—è¡¨ ====================
# ğŸ¯ ç¡¬ç¼–ç æŒ‡å®šè®­ç»ƒèŒƒå›´ï¼ˆä¿®æ”¹å¾ªç¯èŒƒå›´å³å¯ï¼‰
SOURCE_CONS = range(1, 7)  # ä¿®æ”¹è¿™é‡Œï¼šrange(1, 7) è¡¨ç¤ºcon1-con6
TARGET_CONS = range(1, 7)  # ä¿®æ”¹è¿™é‡Œï¼šrange(1, 7) è¡¨ç¤ºcon1-con6

if args.dataset:
    datasets = [args.dataset]
else:
    # è‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†åˆ—è¡¨
    datasets = []
    for src in SOURCE_CONS:
        for tgt in TARGET_CONS:
            datasets.append(f"con{src}con{tgt}Sensor")
    
    print(f"\nğŸ¯ è®­ç»ƒèŒƒå›´: {len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"  æºæµ“åº¦: con{min(SOURCE_CONS)}-con{max(SOURCE_CONS)}")
    print(f"  ç›®æ ‡æµ“åº¦: con{min(TARGET_CONS)}-con{max(TARGET_CONS)}")
    print(f"  æ•°æ®é›†åˆ—è¡¨: {datasets}")
    print()

# ==================== è®­ç»ƒæ¯ä¸ªæ•°æ®é›† ====================
for DATASET_NAME in datasets:
    print("\n" + "="*80)
    print(f"å¼€å§‹è®­ç»ƒæ•°æ®é›†: {DATASET_NAME}")
    print("="*80)
    
    MODEL_SAVE_PATH = f"checkpoints/best_lstm_llm_{DATASET_NAME}.pth"
    log_filename = f"logs/{DATASET_NAME}_lstm_llm_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é‡æ–°é…ç½®logger
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    
    logger.info("="*60)
    logger.info(f"âš¡ LSTM-LLMåˆ†ç±»æ¨¡å‹ - {DATASET_NAME}")
    logger.info("="*60)
    logger.info(f"è®¾å¤‡: {device}")
    logger.info(f"æ—¥å¿—: {log_filename}")
    
    # ==================== 1. æ•°æ®åŠ è½½ ====================
    logger.info("\n" + "="*60)
    logger.info("1. åŠ è½½ProLLMæ•°æ®")
    logger.info("="*60)
    
    train_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_x.npy"
    train_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_y.npy"
    test_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_x.npy"
    test_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_y.npy"
    
    X_train = np.load(train_x_path)  # (N, channels, length)
    y_train = np.load(train_y_path)
    X_test = np.load(test_x_path)
    y_test = np.load(test_y_path)
    
    # å°†æ ‡ç­¾æ˜ å°„åˆ°0-basedç´¢å¼• (CrossEntropyLossè¦æ±‚)
    y_train = y_train - 1
    y_test = y_test - 1
    
    # è·å–ç±»åˆ«æ•°
    num_classes = len(np.unique(y_train))
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†: {X_train.shape}, æ ‡ç­¾: {y_train.shape}")
    logger.info(f"  æµ‹è¯•é›†: {X_test.shape}, æ ‡ç­¾: {y_test.shape}")
    logger.info(f"  ç±»åˆ«æ•°: {num_classes}")
    logger.info(f"  é€šé“æ•°: {X_train.shape[1]}, åºåˆ—é•¿åº¦: {X_train.shape[2]}")
    
    # ==================== 2. åŠ è½½ç¦»çº¿Embeddings ====================
    logger.info("\n" + "="*60)
    logger.info("2. åˆå§‹åŒ–PromptHandlerå¹¶é¢„åŠ è½½embeddings")
    logger.info("="*60)
    
    prompt_handler = PromptHandler(
        tokenizer_path="FacebookAI/roberta-base",
        llm_path="FacebookAI/roberta-base",
        device=device,
        max_length=768,
        representation=PROMPT_REPRESENTATION
    )
    
    logger.info(f"âœ“ PromptHandleråˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  è¡¨ç¤ºç±»å‹: {PROMPT_REPRESENTATION}")
    
    # ğŸš€ é¢„åŠ è½½æ‰€æœ‰embeddingsåˆ°å†…å­˜ï¼ˆæ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼‰
    train_embeddings = prompt_handler.preload_all_embeddings(DATASET_NAME, is_training=True)
    test_embeddings = prompt_handler.preload_all_embeddings(DATASET_NAME, is_training=False)
    
    # ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    train_embeddings = train_embeddings.to(device)
    test_embeddings = test_embeddings.to(device)
    
    logger.info(f"âœ“ Embeddingsé¢„åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†embeddings: {train_embeddings.shape}")
    logger.info(f"  æµ‹è¯•é›†embeddings: {test_embeddings.shape}")
    
    # ==================== 3. æ•°æ®é›†ç±» ====================
    class ProLLMDataset(Dataset):
        def __init__(self, X, y, indices):
            # X: (N, channels, length) -> ä¿æŒåŸæ ¼å¼ï¼Œä¸permute
            # Patch Embeddingéœ€è¦ (batch, channels, seq_len)
            self.X = torch.FloatTensor(X)  # (N, channels, length)
            self.y = torch.LongTensor(y)
            self.indices = torch.LongTensor(indices)
        
        def __len__(self):
            return len(self.X)
        
        def __getitem__(self, idx):
            return self.X[idx], self.y[idx], self.indices[idx]
    
    train_indices = np.arange(len(X_train))
    test_indices = np.arange(len(X_test))
    
    train_dataset = ProLLMDataset(X_train, y_train, train_indices)
    test_dataset = ProLLMDataset(X_test, y_test, test_indices)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    logger.info(f"\nâœ“ æ•°æ®é›†æ„å»ºå®Œæˆ")
    logger.info(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    logger.info(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # ==================== 4. Patch Embeddingï¼ˆå­¦ä¹ ProLLMï¼‰====================
    class PatchEmbedding(nn.Module):
        """å°†æ—¶åºæ•°æ®åˆ†å‰²æˆpatchå¹¶ç¼–ç """
        def __init__(self, d_model, patch_len, stride, dropout):
            super(PatchEmbedding, self).__init__()
            self.patch_len = patch_len
            self.stride = stride
            self.padding_patch_layer = nn.ReplicationPad1d((0, stride))
            
            # 1Då·ç§¯ç¼–ç patch
            padding = 1 if torch.__version__ >= '1.5.0' else 2
            self.value_embedding = nn.Conv1d(
                in_channels=patch_len, 
                out_channels=d_model,
                kernel_size=3, 
                padding=padding, 
                padding_mode='circular', 
                bias=False
            )
            
            # ä½ç½®ç¼–ç 
            self.position_embedding = nn.Parameter(torch.randn(1, 1000, d_model) * 0.02)
            self.dropout = nn.Dropout(dropout)
            
        def forward(self, x):
            """
            Args:
                x: (batch, channels, seq_len)
            Returns:
                embedded patches, num_patches
            """
            n_vars = x.shape[1]
            
            # Padding
            x = self.padding_patch_layer(x)
            
            # Unfoldæˆpatches: (batch, channels, num_patches, patch_len)
            x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)
            
            # é‡å¡‘: (batch*channels, num_patches, patch_len)
            B, C, num_patches, patch_len = x.shape
            x = x.reshape(B * C, num_patches, patch_len)
            
            # å·ç§¯ç¼–ç : (batch*channels, num_patches, patch_len) -> (batch*channels, d_model, num_patches)
            x = x.permute(0, 2, 1)  # (B*C, patch_len, num_patches)
            x = self.value_embedding(x)  # (B*C, d_model, num_patches)
            x = x.permute(0, 2, 1)  # (B*C, num_patches, d_model)
            
            # ä½ç½®ç¼–ç 
            x = x + self.position_embedding[:, :num_patches, :]
            
            return self.dropout(x), n_vars, num_patches
    
    # ==================== 5. æ¨¡å‹å®šä¹‰ ====================
    class LSTMLLM_Classification(nn.Module):
        def __init__(self, input_size, hidden_size, num_classes, 
                     seq_len, patch_len, stride,
                     llm_hidden_size=768, num_layers=2, dropout=0.3):
            super(LSTMLLM_Classification, self).__init__()
            self.d_model = llm_hidden_size  # 768
            self.seq_len = seq_len
            self.input_size = input_size
            
            # Patch Embeddingï¼ˆå­¦ä¹ ProLLMï¼‰
            self.patch_embedding = PatchEmbedding(
                d_model=self.d_model,
                patch_len=patch_len,
                stride=stride,
                dropout=dropout
            )
            
            # è®¡ç®—patchæ•°é‡
            self.patch_nums = int((seq_len - patch_len) / stride + 2)
            
            # LSTMç¼–ç å™¨ï¼ˆå¤„ç†patchåºåˆ—ï¼‰
            self.lstm = nn.LSTM(
                input_size=self.d_model,  # è¾“å…¥æ˜¯patch embedding
                hidden_size=llm_hidden_size, 
                num_layers=num_layers, 
                dropout=dropout, 
                batch_first=True
            )
            
            # ç»´åº¦è°ƒæ•´å±‚ï¼ˆéœ€è¦è€ƒè™‘æ‰€æœ‰é€šé“ï¼‰
            total_lstm_feats = input_size * self.patch_nums * llm_hidden_size
            self.dim_adjust = nn.Linear(total_lstm_feats, self.d_model)
            self.dropout = nn.Dropout(dropout)
            self.relu = nn.LeakyReLU()
            
            # âœ¨ é—¨æ§èåˆæ¨¡å—ï¼ˆå®Œå…¨å­¦ä¹ ProLLMï¼‰
            self.fusion_gate = nn.Sequential(
                nn.Linear(self.d_model * 2, self.d_model),
                nn.Sigmoid()
            )
            
            # åŠ è½½RoBERTaï¼ˆå­¦ä¹ ProLLMï¼‰
            logger.info("  æ­£åœ¨åŠ è½½RoBERTaæ¨¡å‹...")
            config = RobertaConfig.from_pretrained('roberta-base')
            # ğŸš€ ä½¿ç”¨bfloat16åŠ é€Ÿï¼ˆå¯¹é½ProLLMï¼‰
            self.llm_model = RobertaModel.from_pretrained(
                'roberta-base', 
                config=config,
                torch_dtype=torch.bfloat16
            )
            
            # å†»ç»“å¤§éƒ¨åˆ†å±‚ï¼Œåªè§£å†»æœ€å2å±‚
            for name, param in self.llm_model.named_parameters():
                if 'encoder.layer.10' in name or 'encoder.layer.11' in name or 'pooler' in name:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            
            trainable_params = sum(p.numel() for p in self.llm_model.parameters() if p.requires_grad)
            logger.info(f"  âœ“ RoBERTaåŠ è½½å®Œæˆï¼ˆè§£å†»æœ€å2å±‚ï¼Œå¯è®­ç»ƒå‚æ•°ï¼š{trainable_params:,}ï¼‰")
            
            # LayerNorm + åˆ†ç±»å¤´ï¼ˆå®Œå…¨å­¦ä¹ ProLLMï¼‰
            self.ln_proj = nn.LayerNorm(self.d_model)
            self.mapping = nn.Sequential(
                nn.Linear(self.d_model, num_classes),
                nn.Dropout(dropout)
            )
            
            self._initialize_weights()
        
        def _initialize_weights(self):
            for name, m in self.named_modules():
                if 'llm_model' in name:
                    continue
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        
        def forward(self, x_input, x_llm_prompt):
            """
            1. Patch Embeddingåˆ†å‰²æ—¶åº
            2. LSTMå¤„ç†patchåºåˆ—
            3. é—¨æ§èåˆpromptå’ŒLSTMç‰¹å¾
            4. è¾“å…¥RoBERTa
            5. æ®‹å·®è¿æ¥ + åˆ†ç±»
            
            Args:
                x_input: (batch, channels, seq_len) æ—¶åºè¾“å…¥
                x_llm_prompt: (batch, 1, 768) ç¦»çº¿prompt embeddings
            """
            B = x_input.size(0)
            
            # 1. Patch Embedding
            patch_embeds, n_vars, num_patches = self.patch_embedding(x_input)
            # patch_embeds: (batch*channels, num_patches, d_model)
            
            # 2. LSTMç¼–ç æ¯ä¸ªé€šé“çš„patchåºåˆ—
            # é‡å¡‘ä»¥ä¾¿LSTMå¤„ç†
            patch_embeds = patch_embeds.view(B, n_vars, num_patches, self.d_model)
            
            # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«LSTMç¼–ç ï¼Œç„¶åæ‹¼æ¥
            lstm_outs = []
            for c in range(n_vars):
                channel_patches = patch_embeds[:, c, :, :]  # (batch, num_patches, d_model)
                lstm_out, _ = self.lstm(channel_patches)  # (batch, num_patches, d_model)
                lstm_outs.append(lstm_out)
            
            # æ‹¼æ¥æ‰€æœ‰é€šé“
            output_x = torch.stack(lstm_outs, dim=1)  # (batch, channels, num_patches, d_model)
            output_x = output_x.reshape(B, -1)  # (batch, channels*num_patches*d_model)
            
            # ç»´åº¦è°ƒæ•´
            output_x = self.dim_adjust(output_x)  # (batch, d_model)
            output_x = self.dropout(output_x)
            output_x = output_x.unsqueeze(1)  # (batch, 1, d_model)
            
            # ä¿å­˜æ®‹å·®
            output_x_residual = output_x.clone()
            
            # 3. ç¡®ä¿prompt embeddingsç»´åº¦æ­£ç¡®
            if x_llm_prompt.dim() == 2:
                x_llm_prompt = x_llm_prompt.unsqueeze(1)
            prompt_embeddings = x_llm_prompt
            
            # 4. âœ¨ é—¨æ§èåˆ
            concat_feats = torch.cat([prompt_embeddings, output_x], dim=-1)
            gate = self.fusion_gate(concat_feats)
            
            # dtypeç®¡ç†
            llm_dtype = next(self.llm_model.parameters()).dtype
            prompt_embeddings_llm = prompt_embeddings.to(dtype=llm_dtype)
            output_x_llm = output_x.to(dtype=llm_dtype)
            gate_llm = gate.to(dtype=llm_dtype)
            
            # é—¨æ§èåˆ
            fused_embeds = gate_llm * prompt_embeddings_llm + (1 - gate_llm) * output_x_llm
            
            # 5. è¾“å…¥RoBERTa
            llm_out = self.llm_model(inputs_embeds=fused_embeds.contiguous()).last_hidden_state
            time_series_out = llm_out  # (batch, 1, d_model)
            
            # è½¬å›float32
            time_series_out_f32 = time_series_out.float()
            
            # æ•°å€¼ç¨³å®šæ€§å¤„ç†
            if torch.isnan(time_series_out_f32).any() or torch.isinf(time_series_out_f32).any():
                time_series_out_f32 = torch.nan_to_num(time_series_out_f32, nan=0.0, posinf=10.0, neginf=-10.0)
            
            # 6. æ®‹å·®è¿æ¥ + æ¿€æ´»
            outputs = self.relu(time_series_out_f32.squeeze(1) + output_x_residual.squeeze(1))
            outputs = torch.clamp(outputs, min=-50, max=50)
            
            # 7. LayerNorm + åˆ†ç±»
            outputs = self.ln_proj(outputs)
            logits = self.mapping(outputs)
            
            # æœ€ç»ˆè¾“å‡ºä¿æŠ¤
            logits = torch.clamp(logits, min=-100, max=100)
            
            return logits, gate.mean()
    
    model = LSTMLLM_Classification(
        input_size=X_train.shape[1],  # channels
        hidden_size=LSTM_HIDDEN_SIZE,
        num_classes=num_classes,
        seq_len=X_train.shape[2],  # åºåˆ—é•¿åº¦361
        patch_len=PATCH_LEN,
        stride=STRIDE,
        num_layers=LSTM_NUM_LAYERS,
        dropout=LSTM_DROPOUT
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"\nâœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    # ==================== 5. è®­ç»ƒ ====================
    # å·®å¼‚åŒ–å­¦ä¹ ç‡
    llm_params = []
    other_params = []
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'llm_model' in name:
                llm_params.append(param)
            else:
                other_params.append(param)
    
    optimizer = torch.optim.Adam([
        {'params': other_params, 'lr': LEARNING_RATE},
        {'params': llm_params, 'lr': LLM_LEARNING_RATE}
    ], weight_decay=WEIGHT_DECAY)
    
    criterion = nn.CrossEntropyLoss()
    # å¯¹é½ProLLMï¼šä½¿ç”¨StepLRæ›¿ä»£ReduceLROnPlateau
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=50, gamma=0.8
    )
    
    logger.info("\n" + "="*60)
    logger.info("å¼€å§‹è®­ç»ƒï¼ˆLSTM-LLMåˆ†ç±»ä»»åŠ¡ï¼‰")
    logger.info(f"  ä¸»å­¦ä¹ ç‡: {LEARNING_RATE} | RoBERTaå­¦ä¹ ç‡: {LLM_LEARNING_RATE}")
    logger.info(f"  æ—©åœç­–ç•¥: å‡†ç¡®ç‡>{EARLY_STOP_THRESHOLD:.3f}ä¸”è¿ç»­{EARLY_STOP_PATIENCE}è½®ä¸æå‡")
    logger.info("="*60)
    
    best_test_acc = 0
    no_improve_count = 0  # æ—©åœè®¡æ•°å™¨
    training_start_time = time.time()
    
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_preds, train_trues = [], []
        train_gate_weights = []  # æ”¶é›†é—¨æ§å‚æ•°
        
        for X_batch, y_batch, indices_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddingsï¼ˆè¶…å¿«ï¼ï¼‰
            embeddings = train_embeddings[indices_batch]  # (batch, 1, d_model)
            
            optimizer.zero_grad()
            logits, gate_weight = model(X_batch, embeddings)
            loss = criterion(logits, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.extend(logits.argmax(dim=1).cpu().numpy())
            train_trues.extend(y_batch.cpu().numpy())
            train_gate_weights.append(gate_weight.mean().item())  # è®°å½•å¹³å‡é—¨æ§å€¼
        
        train_loss /= len(train_loader)
        train_acc = accuracy_score(train_trues, train_preds)
        avg_gate = np.mean(train_gate_weights)  # è®¡ç®—epochå¹³å‡é—¨æ§
        
        # æµ‹è¯•
        model.eval()
        test_preds, test_trues = [], []
        
        with torch.no_grad():
            for X_batch, y_batch, indices_batch in test_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                
                # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddings
                embeddings = test_embeddings[indices_batch]
                
                logits, _ = model(X_batch, embeddings)
                test_preds.extend(logits.argmax(dim=1).cpu().numpy())
                test_trues.extend(y_batch.cpu().numpy())
        
        test_acc = accuracy_score(test_trues, test_preds)
        test_f1 = f1_score(test_trues, test_preds, average='weighted')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ & æ—©åœé€»è¾‘
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            save_status = "âœ“"
            no_improve_count = 0  # é‡ç½®è®¡æ•°å™¨
        else:
            save_status = ""
            # åªæœ‰è¾¾åˆ°é˜ˆå€¼åæ‰å¼€å§‹è®¡æ•°
            if test_acc >= EARLY_STOP_THRESHOLD:
                no_improve_count += 1
        
        # è®¡ç®—epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        
        logger.info(
            f"Epoch {epoch+1:3d}/{EPOCHS} | "
            f"TrLoss: {train_loss:.4f} | TrAcc: {train_acc:.4f} | "
            f"TeAcc: {test_acc:.4f} | F1: {test_f1:.4f} | "
            f"Gate: {avg_gate:.4f} | "
            f"Time: {epoch_time:.2f}s | {save_status}"
        )
        
        # ğŸ¯ æ—©åœæ£€æŸ¥1: è¾¾åˆ°å®Œç¾å‡†ç¡®ç‡ï¼Œç›´æ¥åœæ­¢
        if test_acc >= 1.0:
            logger.info(f"\nğŸ‰ å®Œç¾å‡†ç¡®ç‡è¾¾æˆï¼")
            logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} (100%)")
            logger.info(f"  æ— éœ€ç»§ç»­è®­ç»ƒï¼Œæå‰ç»“æŸ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        # ğŸ¯ æ—©åœæ£€æŸ¥2: é«˜å‡†ç¡®ç‡ä½†è¿ç»­ä¸æå‡
        if test_acc >= EARLY_STOP_THRESHOLD and no_improve_count >= EARLY_STOP_PATIENCE:
            logger.info(f"\nâš¡ æ—©åœè§¦å‘ï¼")
            logger.info(f"  å½“å‰å‡†ç¡®ç‡: {test_acc:.4f} (>{EARLY_STOP_THRESHOLD:.3f})")
            logger.info(f"  è¿ç»­ {no_improve_count} ä¸ªepochæ— æå‡")
            logger.info(f"  æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}")
            logger.info(f"  æå‰ç»“æŸè®­ç»ƒ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        # å­¦ä¹ ç‡è¡°å‡ï¼ˆå¯¹é½ProLLMï¼šåœ¨epochç»“æŸæ—¶è°ƒç”¨ï¼‰
        scheduler.step()
    
    # ==================== 6. æœ€ç»ˆè¯„ä¼° ====================
    total_training_time = time.time() - training_start_time
    logger.info(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_training_time:.2f}s ({total_training_time/60:.2f}min)")
    
    logger.info("\n" + "="*60)
    logger.info("æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°")
    logger.info("="*60)
    
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    model.eval()
    
    test_preds, test_trues = [], []
    with torch.no_grad():
        for X_batch, y_batch, indices_batch in test_loader:
            X_batch = X_batch.to(device)
            # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddings
            embeddings = test_embeddings[indices_batch]
            logits, _ = model(X_batch, embeddings)
            test_preds.extend(logits.argmax(dim=1).cpu().numpy())
            test_trues.extend(y_batch.cpu().numpy())
    
    test_acc = accuracy_score(test_trues, test_preds)
    test_f1 = f1_score(test_trues, test_preds, average='weighted')

    logger.info(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½")
    logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"  æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"\n{classification_report(test_trues, test_preds)}")
    logger.info(f"\nâœ“ æ¨¡å‹ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
    
    print(f"\nâœ… æ•°æ®é›† {DATASET_NAME} è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}\n")

print("\n" + "="*80)
print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
print("="*80)
