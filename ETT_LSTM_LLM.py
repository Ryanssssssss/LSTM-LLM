"""
ETDatasetç”µåŠ›è´Ÿè·é¢„æµ‹ï¼šLSTM-LLMæ··åˆæ¨¡å‹ï¼ˆç¦»çº¿Embeddingsç‰ˆæœ¬ï¼‰
ä½¿ç”¨ç”µåŠ›å˜å‹å™¨æ•°æ®å±•ç¤ºLLMåœ¨ç†è§£å¤šç‰¹å¾ç›¸å…³æ€§æ–¹é¢çš„ä¼˜åŠ¿
éœ€è¦å…ˆè¿è¡Œ generate_embeddings_ett.py ç”Ÿæˆembeddings
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
import logging
import os
import json
from datetime import datetime
warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
LOOK_BACK = 96          # å†å²çª—å£ï¼š96å°æ—¶ï¼ˆ4å¤©ï¼‰
N_FUTURE = 24           # é¢„æµ‹çª—å£ï¼š24å°æ—¶ï¼ˆ1å¤©ï¼‰
TRAIN_RATIO = 0.6
VAL_RATIO = 0.2
TEST_RATIO = 0.2

LSTM_HIDDEN_SIZE = 128
LSTM_NUM_LAYERS = 2
LSTM_DROPOUT = 0.2

BATCH_SIZE = 32
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-5
EPOCHS = 100
EARLY_STOP_PATIENCE = 15
GRAD_CLIP_NORM = 1.0

RANDOM_SEED = 42
DATA_FILE = "ETDataset/ETT-small/ETTh1.csv"
EMBEDDING_DIR = "embeddings/ett"  # é¢„ç”Ÿæˆçš„embeddingsç›®å½•
MODEL_SAVE_PATH = "checkpoints/best_ett_lstm_llm.pth"
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

# é…ç½®æ—¥å¿—
os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)
os.makedirs('results', exist_ok=True)
log_filename = f"logs/ett_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(RANDOM_SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

logger.info("="*60)
logger.info("âš¡ ETDatasetç”µåŠ›è´Ÿè·é¢„æµ‹ - LSTM-LLMæ··åˆæ¨¡å‹ï¼ˆç¦»çº¿ç‰ˆï¼‰")
logger.info("="*60)
logger.info(f"è®¾å¤‡: {device}")
logger.info(f"æ—¥å¿—: {log_filename}")
logger.info(f"å†å²çª—å£: {LOOK_BACK}å°æ—¶ | é¢„æµ‹çª—å£: {N_FUTURE}å°æ—¶")
logger.info(f"LSTMç»“æ„: {LSTM_NUM_LAYERS}å±‚ Ã— {LSTM_HIDDEN_SIZE}ç»´")

# ==================== 1. æ•°æ®åŠ è½½ ====================
logger.info("\n" + "="*60)
logger.info("1. åŠ è½½ETDatasetæ•°æ®")
logger.info("="*60)

df = pd.read_csv(DATA_FILE)
df['date'] = pd.to_datetime(df['date'])
logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]}æ¡è®°å½•")

# æå–ç‰¹å¾ï¼ˆå»é™¤æ—¥æœŸåˆ—ï¼‰
feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']
target_col = 'OT'  # æ²¹æ¸©ä½œä¸ºé¢„æµ‹ç›®æ ‡

data = df[feature_cols + [target_col]].values

# åŠ è½½é¢„ä¿å­˜çš„scaler
scaler_mean = np.load(f"{EMBEDDING_DIR}/scaler_mean.npy")
scaler_scale = np.load(f"{EMBEDDING_DIR}/scaler_scale.npy")
scaler = StandardScaler()
scaler.mean_ = scaler_mean
scaler.scale_ = scaler_scale

data_scaled = scaler.transform(data)

logger.info(f"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼ˆä½¿ç”¨é¢„ä¿å­˜çš„scalerï¼‰")

# ==================== 2. æ„å»ºæ—¶åºæ•°æ®é›† ====================
def create_sequences(data, look_back, n_future):
    X, y = [], []
    for i in range(len(data) - look_back - n_future + 1):
        X.append(data[i:i+look_back, :])
        y.append(data[i+look_back:i+look_back+n_future, -1])  # åªé¢„æµ‹OT
    return np.array(X), np.array(y)

X, y = create_sequences(data_scaled, LOOK_BACK, N_FUTURE)
logger.info(f"\nâœ“ æ—¶åºæ•°æ®æ„å»ºå®Œæˆ")
logger.info(f"  X shape: {X.shape} (æ ·æœ¬æ•°, æ—¶é—´æ­¥, ç‰¹å¾æ•°)")
logger.info(f"  y shape: {y.shape} (æ ·æœ¬æ•°, é¢„æµ‹æ­¥æ•°)")

# æ•°æ®åˆ’åˆ†
train_size = int(len(X) * TRAIN_RATIO)
val_size = int(len(X) * VAL_RATIO)

X_train = X[:train_size]
y_train = y[:train_size]
X_val = X[train_size:train_size+val_size]
y_val = y[train_size:train_size+val_size]
X_test = X[train_size+val_size:]
y_test = y[train_size+val_size:]

logger.info(f"âœ“ æ•°æ®é›†åˆ’åˆ†å®Œæˆ")
logger.info(f"  è®­ç»ƒé›†: {len(X_train)}æ ·æœ¬ ({TRAIN_RATIO*100:.0f}%)")
logger.info(f"  éªŒè¯é›†: {len(X_val)}æ ·æœ¬ ({VAL_RATIO*100:.0f}%)")
logger.info(f"  æµ‹è¯•é›†: {len(X_test)}æ ·æœ¬ ({TEST_RATIO*100:.0f}%)")

# ==================== 3. åŠ è½½é¢„ç”Ÿæˆçš„Embeddings ====================
logger.info("\n" + "="*60)
logger.info("3. åŠ è½½é¢„ç”Ÿæˆçš„GPT-2 Embeddings")
logger.info("="*60)

def load_embeddings(save_dir):
    """åŠ è½½åˆ†å—ä¿å­˜çš„embeddings"""
    embeddings = []
    i = 0
    while os.path.exists(f"{save_dir}/embeddings_{i}.npy"):
        chunk = np.load(f"{save_dir}/embeddings_{i}.npy")
        embeddings.append(chunk)
        i += 1
    if len(embeddings) == 0:
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°embeddingsæ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œ: python generate_embeddings_ett.py"
        )
    return np.vstack(embeddings)

train_embeddings = load_embeddings(f"{EMBEDDING_DIR}/train")
val_embeddings = load_embeddings(f"{EMBEDDING_DIR}/val")
test_embeddings = load_embeddings(f"{EMBEDDING_DIR}/test")

logger.info(f"âœ“ EmbeddingsåŠ è½½å®Œæˆ")
logger.info(f"  è®­ç»ƒé›†: {train_embeddings.shape}")
logger.info(f"  éªŒè¯é›†: {val_embeddings.shape}")
logger.info(f"  æµ‹è¯•é›†: {test_embeddings.shape}")

# éªŒè¯å°ºå¯¸åŒ¹é…
assert len(train_embeddings) == len(X_train), "è®­ç»ƒé›†å°ºå¯¸ä¸åŒ¹é…ï¼"
assert len(val_embeddings) == len(X_val), "éªŒè¯é›†å°ºå¯¸ä¸åŒ¹é…ï¼"
assert len(test_embeddings) == len(X_test), "æµ‹è¯•é›†å°ºå¯¸ä¸åŒ¹é…ï¼"
logger.info(f"âœ“ æ•°æ®å°ºå¯¸éªŒè¯é€šè¿‡")

# ==================== 4. æ•°æ®é›†ç±» ====================
class ETTDataset(Dataset):
    def __init__(self, X, y, embeddings):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.embeddings = torch.FloatTensor(embeddings)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.embeddings[idx], self.y[idx]

train_dataset = ETTDataset(X_train, y_train, train_embeddings)
val_dataset = ETTDataset(X_val, y_val, val_embeddings)
test_dataset = ETTDataset(X_test, y_test, test_embeddings)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ==================== 5. æ¨¡å‹å®šä¹‰ ====================
class LSTMLLM_ETT(nn.Module):
    def __init__(self, lstm_input_size, lstm_hidden_size, llm_hidden_size, 
                 output_steps, num_layers=2, dropout=0.2):
        super(LSTMLLM_ETT, self).__init__()
        
        # LSTMåˆ†æ”¯ï¼šç¼–ç æ—¶åºæ¨¡å¼
        self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size, 
                           num_layers=num_layers, dropout=dropout, 
                           batch_first=True)
        
        # LLMæŠ•å½±å±‚ï¼šå°†768ç»´é™åˆ°128ç»´
        self.llm_projector = nn.Sequential(
            nn.Linear(llm_hidden_size, lstm_hidden_size),
            nn.LayerNorm(lstm_hidden_size),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # é—¨æ§èåˆå•å…ƒï¼ˆæ”¹è¿›ç‰ˆï¼‰ï¼šä½¿ç”¨å¯å­¦ä¹ çš„æƒè¡¡æœºåˆ¶
        self.fusion_gate = nn.Sequential(
            nn.Linear(lstm_hidden_size * 2, lstm_hidden_size),
            nn.Tanh(),
            nn.Linear(lstm_hidden_size, lstm_hidden_size),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(lstm_hidden_size * 2, lstm_hidden_size),
            nn.LayerNorm(lstm_hidden_size),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # é¢„æµ‹å¤´
        self.predictor = nn.Sequential(
            nn.Linear(lstm_hidden_size, 256),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(128, output_steps)
        )
        
        # ä½¿ç”¨Xavieråˆå§‹åŒ–æ¥é¿å…æ¢¯åº¦é—®é¢˜
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡ï¼Œé¿å…åå‘æŸä¸€åˆ†æ”¯"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x_lstm, x_llm):
        # 1. LSTMç¼–ç æ—¶åºç‰¹å¾
        lstm_out, _ = self.lstm(x_lstm)  # (batch, seq_len, 128)
        lstm_feat = lstm_out[:, -1, :]   # (batch, 128)
        
        # 2. LLMæŠ•å½±åˆ°ç›¸åŒç»´åº¦
        llm_feat = self.llm_projector(x_llm)  # (batch, 128)
        
        # 3. ç‰¹å¾çº§åˆ«çš„é—¨æ§èåˆ
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æƒé‡ï¼ˆè€Œéå…¨å±€å•ä¸€æƒé‡ï¼‰
        combined = torch.cat([lstm_feat, llm_feat], dim=1)  # (batch, 256)
        gate = self.fusion_gate(combined)  # (batch, 128)
        
        # é€ç»´åº¦åŠ æƒèåˆ
        fused = gate * lstm_feat + (1 - gate) * llm_feat  # (batch, 128)
        
        # 4. å†æ¬¡èåˆåŸå§‹ç‰¹å¾ï¼ˆè·³è·ƒè¿æ¥ï¼‰
        fused_enhanced = self.fusion_layer(
            torch.cat([fused, lstm_feat], dim=1)
        )  # (batch, 128)
        
        # 5. é¢„æµ‹
        output = self.predictor(fused_enhanced)  # (batch, output_steps)
        
        # è¿”å›å¹³å‡gateæƒé‡ä½œä¸ºå¯è§£é‡Šæ€§æŒ‡æ ‡
        avg_gate = gate.mean(dim=1, keepdim=True)  # (batch, 1)
        return output, avg_gate

model = LSTMLLM_ETT(
    lstm_input_size=7,
    lstm_hidden_size=LSTM_HIDDEN_SIZE,
    llm_hidden_size=768,
    output_steps=N_FUTURE,
    num_layers=LSTM_NUM_LAYERS,
    dropout=LSTM_DROPOUT
).to(device)

logger.info(f"\nâœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
logger.info(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ==================== 6. è®­ç»ƒ ====================
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
criterion = nn.MSELoss()

logger.info("\n" + "="*60)
logger.info("å¼€å§‹è®­ç»ƒ")
logger.info("="*60)

best_val_loss = float('inf')
patience_counter = 0
gate_weights_history = []

for epoch in range(EPOCHS):
    # è®­ç»ƒ
    model.train()
    train_loss = 0
    for X_batch, emb_batch, y_batch in train_loader:
        X_batch, emb_batch, y_batch = X_batch.to(device), emb_batch.to(device), y_batch.to(device)
        
        optimizer.zero_grad()
        pred, gate_weight = model(X_batch, emb_batch)
        loss = criterion(pred, y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
        optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # éªŒè¯
    model.eval()
    val_loss = 0
    epoch_gate_weights = []
    with torch.no_grad():
        for X_batch, emb_batch, y_batch in val_loader:
            X_batch, emb_batch, y_batch = X_batch.to(device), emb_batch.to(device), y_batch.to(device)
            pred, gate_weight = model(X_batch, emb_batch)
            loss = criterion(pred, y_batch)
            val_loss += loss.item()
            epoch_gate_weights.append(gate_weight.mean().item())
    
    val_loss /= len(val_loader)
    avg_gate_weight = np.mean(epoch_gate_weights)
    gate_weights_history.append(avg_gate_weight)
    
    scheduler.step(val_loss)
    
    # æ—©åœ
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        patience_counter = 0
        save_status = f"âœ“ å·²ä¿å­˜"
    else:
        patience_counter += 1
        save_status = f"æœªä¿å­˜ (patience: {patience_counter}/{EARLY_STOP_PATIENCE})"
    
    if (epoch + 1) % 5 == 0:
        logger.info(
            f"Epoch {epoch+1}/{EPOCHS} | "
            f"Train Loss: {train_loss:.6f} | "
            f"Val Loss: {val_loss:.6f} | "
            f"Gateæƒé‡: {avg_gate_weight:.3f} | "
            f"{save_status}"
        )
    
    if patience_counter >= EARLY_STOP_PATIENCE:
        logger.info(f"\næ—©åœè§¦å‘ï¼Œæœ€ä½³éªŒè¯loss: {best_val_loss:.6f}")
        break

# ==================== 7. æµ‹è¯• ====================
logger.info("\n" + "="*60)
logger.info("7. æµ‹è¯•é›†è¯„ä¼°")
logger.info("="*60)

model.load_state_dict(torch.load(MODEL_SAVE_PATH))
model.eval()

all_preds = []
all_trues = []
test_gate_weights = []

with torch.no_grad():
    for X_batch, emb_batch, y_batch in test_loader:
        X_batch, emb_batch, y_batch = X_batch.to(device), emb_batch.to(device), y_batch.to(device)
        pred, gate_weight = model(X_batch, emb_batch)
        all_preds.append(pred.cpu().numpy())
        all_trues.append(y_batch.cpu().numpy())
        test_gate_weights.append(gate_weight.cpu().numpy())

y_pred = np.vstack(all_preds)
y_true = np.vstack(all_trues)
test_gate_weights = np.vstack(test_gate_weights)

# åæ ‡å‡†åŒ–ï¼ˆåªé’ˆå¯¹OTåˆ—ï¼‰
ot_scaler = StandardScaler()
ot_scaler.mean_ = scaler.mean_[-1]
ot_scaler.scale_ = scaler.scale_[-1]

y_pred_original = ot_scaler.inverse_transform(y_pred)
y_true_original = ot_scaler.inverse_transform(y_true)

# è®¡ç®—æŒ‡æ ‡
mse = mean_squared_error(y_true_original, y_pred_original)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_true_original, y_pred_original)
r2 = r2_score(y_true_original.flatten(), y_pred_original.flatten())

logger.info("="*60)
logger.info("ğŸ“Š æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡")
logger.info("="*60)
logger.info(f"  RMSE: {rmse:.4f}Â°C")
logger.info(f"  MAE:  {mae:.4f}Â°C")
logger.info(f"  RÂ²:   {r2:.4f}")
logger.info(f"  å¹³å‡Gateæƒé‡: {test_gate_weights.mean():.3f} (0.5è¡¨ç¤ºLSTMä¸LLMè´¡çŒ®ç›¸å½“)")
logger.info("="*60)

# å¯è§†åŒ–
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

fig, axes = plt.subplots(2, 1, figsize=(14, 8))

# é¢„æµ‹å¯¹æ¯”
axes[0].plot(y_true_original[:200, 0], label='çœŸå®å€¼', alpha=0.7)
axes[0].plot(y_pred_original[:200, 0], label='é¢„æµ‹å€¼', alpha=0.7)
axes[0].set_title(f'ETTç”µåŠ›è´Ÿè·é¢„æµ‹ (LSTM-LLM) | RMSE={rmse:.4f}Â°C, RÂ²={r2:.4f}')
axes[0].set_xlabel('æ—¶é—´æ­¥')
axes[0].set_ylabel('æ²¹æ¸© (Â°C)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Gateæƒé‡åˆ†å¸ƒ
axes[1].hist(test_gate_weights.flatten(), bins=50, alpha=0.7, edgecolor='black')
axes[1].axvline(test_gate_weights.mean(), color='red', linestyle='--', 
                label=f'å‡å€¼={test_gate_weights.mean():.3f}')
axes[1].set_title('ç‰¹å¾çº§Gateæƒé‡åˆ†å¸ƒï¼ˆ0.5è¡¨ç¤ºLSTMä¸LLMè´¡çŒ®ç›¸å½“ï¼‰')
axes[1].set_xlabel('Gateæƒé‡')
axes[1].set_ylabel('é¢‘æ•°')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/ett_lstm_llm_results.png', dpi=150, bbox_inches='tight')
logger.info(f"\nâœ“ ç»“æœå›¾ä¿å­˜è‡³: results/ett_lstm_llm_results.png")

logger.info(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
