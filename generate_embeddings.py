"""
ğŸŒŸ ç¦»çº¿ç”ŸæˆPrompt Embeddingsï¼ˆæµ‹è¯•ä¿®æ”¹ï¼‰
å‚è€ƒProLLMæ¶æ„ï¼Œé¢„å…ˆè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„GPT-2 embeddingsï¼Œé¿å…è®­ç»ƒæ—¶é‡å¤ç¼–ç 
"""
import numpy as np
import pandas as pd
import torch
import os
import h5py
from tqdm import tqdm
from transformers import GPT2Model, GPT2Tokenizer
from sklearn.preprocessing import MinMaxScaler

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== 1. æ•°æ®åŠ è½½ï¼ˆä¸ä¸»è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰ ====================
print("="*80)
print("1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†")
print("="*80)

df = pd.read_excel("data/æ•°æ®åˆ—è¡¨ï¼ˆ20240317~20240505ï¼‰.xlsx")
df = df.iloc[::-1].reset_index(drop=True)
df = df.loc[:,['åœŸå£¤æ¸©åº¦','ç©ºæ°”æ¸©åº¦','ç©ºæ°”æ¹¿åº¦']]

for i in range(len(df)):
    df.iloc[i,0] = float(df.iloc[i,0][:-1])
    df.iloc[i,1] = float(df.iloc[i,1][:-1])
    df.iloc[i,2] = float(df.iloc[i,2][:-1])

print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

# å½’ä¸€åŒ–
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(df.values)

# ==================== 2. æ—¶åºæ•°æ®æ„å»º ====================
print("\n" + "="*60)
print("2. æ—¶åºæ•°æ®æ„å»º")
print("="*60)

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    n_vars = 1 if type(data) is list else data.shape[1]
    df = pd.DataFrame(data)
    cols, names = list(), list()
    
    for i in range(n_in, 0, -1):
        cols.append(df.shift(i))
        names += [f'var{j+1}(t-{i})' for j in range(n_vars)]
    
    for i in range(0, n_out):
        cols.append(df.shift(-i))
        if i == 0:
            names += [f'var{j+1}(t)' for j in range(n_vars)]
        else:
            names += [f'var{j+1}(t+{i})' for j in range(n_vars)]
    
    agg = pd.concat(cols, axis=1)
    agg.columns = names
    if dropnan:
        agg.dropna(inplace=True)
    return agg

look_back = 10
n_future = 6

supervised_data = series_to_supervised(scaled_data, n_in=look_back, n_out=n_future)
supervised_data = supervised_data.reset_index(drop=True)

input_cols = [col for col in supervised_data.columns if '(t-' in col or col == 'var1(t)']
X = supervised_data[input_cols].values

print(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶ X: {X.shape}")

# ==================== 3. æ•°æ®é›†åˆ’åˆ†ï¼ˆä¸ä¸»è„šæœ¬ä¸€è‡´ï¼‰ ====================
print("\n" + "="*60)
print("3. æ•°æ®é›†åˆ’åˆ†")
print("="*60)

train_size = int(len(X) * 0.7)
val_size = int(len(X) * 0.1)

X_train = X[:train_size]
X_val = X[train_size:train_size+val_size]
X_test = X[train_size+val_size:]

print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
print(f"éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")

# ==================== 4. Promptç”Ÿæˆå™¨ ====================
print("\n" + "="*60)
print("4. åˆå§‹åŒ–Promptç”Ÿæˆå™¨")
print("="*60)

class PromptGenerator:
    """ç”Ÿæˆæè¿°æ€§prompt"""
    def __init__(self, feature_names, look_back=10):
        self.feature_names = feature_names
        self.look_back = look_back
    
    def generate_prompt(self, x_batch, scaler):
        batch_size = x_batch.shape[0]
        prompts = []
        
        for i in range(batch_size):
            sample_features = []
            for step in range(self.look_back):
                start_idx = step * 3
                sample_features.append(x_batch[i, start_idx:start_idx+3])
            sample = np.array(sample_features)
            
            sample_real = scaler.inverse_transform(sample)
            
            soil_temp = sample_real[:, 0]
            air_temp = sample_real[:, 1]
            humidity = sample_real[:, 2]
            
            soil_trend = np.polyfit(np.arange(len(soil_temp)), soil_temp, 1)[0]
            air_trend = np.polyfit(np.arange(len(air_temp)), air_temp, 1)[0]
            
            prompt = (
                f"<|greenhouse_prediction|>åœ°ç†ä½ç½®ï¼šå¹¿ä¸œçœäº‘æµ®å¸‚ï¼ˆåŒ—çº¬22Â°22â€²-23Â°19â€²ï¼Œä¸œç»111Â°03â€²-112Â°31â€²ï¼‰ã€‚"
                f"å†å²æ•°æ®ï¼šè¿‡å»{self.look_back}ä¸ªæ—¶é—´æ­¥çš„ç¯å¢ƒç›‘æµ‹æ•°æ®ã€‚"
                f"åœŸå£¤æ¸©åº¦èŒƒå›´ï¼š{soil_temp.min():.1f}â„ƒ-{soil_temp.max():.1f}â„ƒï¼Œå¹³å‡{soil_temp.mean():.1f}â„ƒï¼Œ"
                f"å˜åŒ–è¶‹åŠ¿ï¼š{'å‡æ¸©' if soil_trend > 0 else 'é™æ¸©'}{abs(soil_trend):.2f}â„ƒ/æ­¥ã€‚"
                f"ç©ºæ°”æ¸©åº¦èŒƒå›´ï¼š{air_temp.min():.1f}â„ƒ-{air_temp.max():.1f}â„ƒï¼Œå¹³å‡{air_temp.mean():.1f}â„ƒï¼Œ"
                f"å˜åŒ–è¶‹åŠ¿ï¼š{'å‡æ¸©' if air_trend > 0 else 'é™æ¸©'}{abs(air_trend):.2f}â„ƒ/æ­¥ã€‚"
                f"ç©ºæ°”æ¹¿åº¦èŒƒå›´ï¼š{humidity.min():.1f}%-{humidity.max():.1f}%ï¼Œå¹³å‡{humidity.mean():.1f}%ã€‚"
                f"ä»»åŠ¡ï¼šåŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œé¢„æµ‹æœªæ¥{n_future}æ­¥ï¼ˆ120åˆ†é’Ÿï¼‰çš„åœŸå£¤æ¸©åº¦å˜åŒ–ã€‚<|endoftext|>"
            )
            prompts.append(prompt)
        
        return prompts

feature_names = ['åœŸå£¤æ¸©åº¦', 'ç©ºæ°”æ¸©åº¦', 'ç©ºæ°”æ¹¿åº¦']
prompt_gen = PromptGenerator(feature_names, look_back=look_back)

# ==================== 5. åŠ è½½GPT-2æ¨¡å‹ ====================
print("\n" + "="*60)
print("5. åŠ è½½GPT-2æ¨¡å‹")
print("="*60)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2Model.from_pretrained('gpt2').to(device)
gpt2_model.eval()

print("âœ“ GPT-2æ¨¡å‹åŠ è½½å®Œæˆ - æµ‹è¯•ç‰ˆæœ¬")

# ==================== 6. ç”Ÿæˆå¹¶ä¿å­˜Embeddings ====================
print("\n" + "="*60)
print("6. ç”ŸæˆPrompt Embeddings")
print("="*60)

def generate_embeddings(X_data, split_name, batch_size=32):
    """
    ä¸ºæ•°æ®é›†ç”Ÿæˆembeddingså¹¶ä¿å­˜ä¸ºh5æ–‡ä»¶
    """
    save_dir = f"embeddings/greenhouse_soil/{split_name}"
    os.makedirs(save_dir, exist_ok=True)
    
    num_samples = len(X_data)
    print(f"\nå¤„ç† {split_name} é›†: {num_samples} æ ·æœ¬")
    
    with torch.no_grad():
        for start_idx in tqdm(range(0, num_samples, batch_size), 
                              desc=f"ç”Ÿæˆ{split_name}é›†embeddings"):
            end_idx = min(start_idx + batch_size, num_samples)
            batch_x = X_data[start_idx:end_idx]
            
            # ç”Ÿæˆprompts
            prompts = prompt_gen.generate_prompt(batch_x, scaler)
            
            # Tokenize
            encoded = tokenizer(
                prompts,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            ).to(device)
            
            # è·å–GPT-2 embeddings
            outputs = gpt2_model(**encoded)
            embeddings = outputs.last_hidden_state  # (batch, seq_len, 768)
            
            # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„embeddingï¼ˆpooledï¼‰
            attention_mask = encoded['attention_mask']
            lengths = attention_mask.sum(dim=1) - 1  # æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„ä½ç½®
            
            for i, global_idx in enumerate(range(start_idx, end_idx)):
                last_token_idx = lengths[i].item()
                embedding = embeddings[i, last_token_idx, :]  # (768,)
                
                # ä¿å­˜ä¸ºh5æ–‡ä»¶
                h5_path = os.path.join(save_dir, f"{global_idx}.h5")
                with h5py.File(h5_path, 'w') as hf:
                    hf.create_dataset('embedding', 
                                    data=embedding.cpu().numpy().astype('float32'))
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    print(f"âœ“ {split_name}é›†embeddingså·²ä¿å­˜è‡³ {save_dir}/")

# ç”Ÿæˆæ‰€æœ‰æ•°æ®é›†çš„embeddings (æµ‹è¯•ä¿®æ”¹ï¼šè°ƒæ•´batch_size)
generate_embeddings(X_train, 'train', batch_size=16)
generate_embeddings(X_val, 'val', batch_size=16)
generate_embeddings(X_test, 'test', batch_size=16)

# ==================== 7. éªŒè¯ç”Ÿæˆçš„embeddings ====================
print("\n" + "="*60)
print("7. éªŒè¯Embeddings")
print("="*60)

def verify_embeddings(split_name, expected_count):
    save_dir = f"embeddings/greenhouse_soil/{split_name}"
    files = [f for f in os.listdir(save_dir) if f.endswith('.h5')]
    
    print(f"{split_name}: ç”Ÿæˆäº† {len(files)} ä¸ªæ–‡ä»¶ï¼ˆé¢„æœŸ {expected_count}ï¼‰")
    
    if len(files) > 0:
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶
        sample_path = os.path.join(save_dir, files[0])
        with h5py.File(sample_path, 'r') as hf:
            emb = hf['embedding'][:]
            print(f"  æ ·æœ¬embeddingå½¢çŠ¶: {emb.shape}, dtype: {emb.dtype}")

verify_embeddings('train', len(X_train))
verify_embeddings('val', len(X_val))
verify_embeddings('test', len(X_test))

print("\n" + "="*60)
print("âœ“ Embeddingç”Ÿæˆå®Œæˆï¼(æµ‹è¯•è¿è¡Œ)")
print("="*60)
# print(f"æ—¥å¿—æ–‡ä»¶: {log_filename}")  # æµ‹è¯•ï¼šæ³¨é‡Šæ‰æœªå®šä¹‰çš„å˜é‡
