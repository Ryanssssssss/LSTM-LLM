"""
çº¯LLMåˆ†ç±»æ¨¡å‹ - æ¶ˆèå®éªŒï¼ˆåªç”¨Prompt Embeddingsï¼‰
ä¸ä½¿ç”¨LSTMæ—¶åºç¼–ç ï¼Œç›´æ¥ç”¨é¢„è®­ç»ƒçš„Prompt Embeddingsè¿›è¡Œåˆ†ç±»

âš ï¸ æ³¨æ„ï¼šç”±äºä½¿ç”¨pooled_last_tokenæ¨¡å¼ï¼ˆå•tokenï¼‰ï¼Œç§»é™¤äº†RoBERTaå±‚
- Prompt Embeddingså·²ç»æ˜¯RoBERTaç¼–ç åçš„æ± åŒ–ç»“æœ
- å†è¾“å…¥RoBERTa(å•token)ä¼šé€€åŒ–ä¸ºæ’ç­‰æ˜ å°„ï¼Œæ— æ³•å­¦ä¹ 
- æ”¹ä¸ºç›´æ¥åœ¨embeddingsä¸Šæ„å»ºåˆ†ç±»å™¨
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
from transformers import RobertaModel, RobertaConfig
import warnings
import logging
import os
import sys
import time
from datetime import datetime

# å¯¼å…¥æ ¹ç›®å½•çš„ PromptHandler
from prompt_handler import PromptHandler

warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
import argparse

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--dataset", type=str, default=None, help="æ•°æ®é›†åç§°ï¼Œä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰con*Sensor")
parser.add_argument("--device", type=str, default="cuda", choices=["cpu", "cuda"], help="è®¡ç®—è®¾å¤‡")
args = parser.parse_args()

DATA_DIR = "ProLLM/con_normalized"

BATCH_SIZE = 4  # å¯¹é½ProLLMï¼š16 -> 4
LEARNING_RATE = 0.001  # ğŸ”¥ æé«˜å­¦ä¹ ç‡ï¼ˆä»0.0001åˆ°0.001ï¼‰ï¼Œå› ä¸ºåªè®­ç»ƒè½»é‡åˆ†ç±»å™¨
WEIGHT_DECAY = 1e-4  
EPOCHS = 50
GRAD_CLIP_NORM = 1.0
DROPOUT = 0.3  # ğŸ”¥ å¢åŠ dropoutï¼ˆä»0.1åˆ°0.3ï¼‰ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

# æ—©åœé…ç½®
EARLY_STOP_PATIENCE = 10
EARLY_STOP_THRESHOLD = 0.995

RANDOM_SEED = 42
PROMPT_REPRESENTATION = "pooled_last_token"
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(RANDOM_SEED)
device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

# ==================== è·å–æ•°æ®é›†åˆ—è¡¨ ====================
# ğŸ¯ ç¡¬ç¼–ç æŒ‡å®šè®­ç»ƒèŒƒå›´ï¼ˆä¿®æ”¹å¾ªç¯èŒƒå›´å³å¯ï¼‰
SOURCE_CONS = range(1, 7)  # ä¿®æ”¹è¿™é‡Œï¼šrange(1, 7) è¡¨ç¤ºcon1-con6
TARGET_CONS = range(1, 7)  # ä¿®æ”¹è¿™é‡Œï¼šrange(1, 7) è¡¨ç¤ºcon1-con6

if args.dataset:
    datasets = [args.dataset]
else:
    # è‡ªåŠ¨ç”Ÿæˆæ•°æ®é›†åˆ—è¡¨
    datasets = []
    for src in SOURCE_CONS:
        for tgt in TARGET_CONS:
            datasets.append(f"con{src}con{tgt}Sensor")
    
    print(f"\nğŸ¯ è®­ç»ƒèŒƒå›´: {len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"  æºæµ“åº¦: con{min(SOURCE_CONS)}-con{max(SOURCE_CONS)}")
    print(f"  ç›®æ ‡æµ“åº¦: con{min(TARGET_CONS)}-con{max(TARGET_CONS)}")
    print(f"  æ•°æ®é›†åˆ—è¡¨: {datasets}")
    print()

# ==================== è®­ç»ƒæ¯ä¸ªæ•°æ®é›† ====================
for DATASET_NAME in datasets:
    print("\n" + "="*80)
    print(f"å¼€å§‹è®­ç»ƒæ•°æ®é›†: {DATASET_NAME}")
    print("="*80)
    
    MODEL_SAVE_PATH = f"checkpoints/best_llm_only_{DATASET_NAME}.pth"
    log_filename = f"logs/{DATASET_NAME}_llm_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é‡æ–°é…ç½®logger
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    
    logger.info("="*60)
    logger.info(f"âš¡ çº¯LLMåˆ†ç±»æ¨¡å‹ï¼ˆæ¶ˆèå®éªŒï¼‰- {DATASET_NAME}")
    logger.info("="*60)
    logger.info(f"è®¾å¤‡: {device}")
    logger.info(f"æ—¥å¿—: {log_filename}")
    
    # ==================== 1. æ•°æ®åŠ è½½ ====================
    logger.info("\n" + "="*60)
    logger.info("1. åŠ è½½ProLLMæ•°æ®")
    logger.info("="*60)
    
    train_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_x.npy"
    train_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_y.npy"
    test_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_x.npy"
    test_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_y.npy"
    
    X_train = np.load(train_x_path)  # (N, channels, length)
    y_train = np.load(train_y_path)
    X_test = np.load(test_x_path)
    y_test = np.load(test_y_path)
    
    # å°†æ ‡ç­¾æ˜ å°„åˆ°0-basedç´¢å¼• (CrossEntropyLossè¦æ±‚)
    y_train = y_train - 1
    y_test = y_test - 1
    
    # è·å–ç±»åˆ«æ•°
    num_classes = len(np.unique(y_train))
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†: {X_train.shape}, æ ‡ç­¾: {y_train.shape}")
    logger.info(f"  æµ‹è¯•é›†: {X_test.shape}, æ ‡ç­¾: {y_test.shape}")
    logger.info(f"  ç±»åˆ«æ•°: {num_classes}")
    logger.info(f"  é€šé“æ•°: {X_train.shape[1]}, åºåˆ—é•¿åº¦: {X_train.shape[2]}")
    
    # ==================== 2. åŠ è½½ç¦»çº¿Embeddings ====================
    logger.info("\n" + "="*60)
    logger.info("2. åˆå§‹åŒ–PromptHandlerå¹¶é¢„åŠ è½½embeddings")
    logger.info("="*60)
    
    prompt_handler = PromptHandler(
        tokenizer_path="FacebookAI/roberta-base",
        llm_path="FacebookAI/roberta-base",
        device=device,
        max_length=768,
        representation=PROMPT_REPRESENTATION
    )
    
    logger.info(f"âœ“ PromptHandleråˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  è¡¨ç¤ºç±»å‹: {PROMPT_REPRESENTATION}")
    
    # ğŸš€ é¢„åŠ è½½æ‰€æœ‰embeddingsåˆ°å†…å­˜ï¼ˆæ˜¾è‘—åŠ é€Ÿè®­ç»ƒï¼‰
    train_embeddings = prompt_handler.preload_all_embeddings(DATASET_NAME, is_training=True)
    test_embeddings = prompt_handler.preload_all_embeddings(DATASET_NAME, is_training=False)
    
    # ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    train_embeddings = train_embeddings.to(device)
    test_embeddings = test_embeddings.to(device)
    
    logger.info(f"âœ“ Embeddingsé¢„åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†embeddings: {train_embeddings.shape}")
    logger.info(f"  æµ‹è¯•é›†embeddings: {test_embeddings.shape}")
    
    # ==================== 3. æ•°æ®é›†ç±» ====================
    class LLMOnlyDataset(Dataset):
        def __init__(self, y, indices):
            # åªéœ€è¦æ ‡ç­¾å’Œç´¢å¼•ï¼Œä¸éœ€è¦Xï¼ˆæ—¶åºæ•°æ®ï¼‰
            self.y = torch.LongTensor(y)
            self.indices = torch.LongTensor(indices)
        
        def __len__(self):
            return len(self.y)
        
        def __getitem__(self, idx):
            return self.y[idx], self.indices[idx]
    
    train_indices = np.arange(len(X_train))
    test_indices = np.arange(len(X_test))
    
    train_dataset = LLMOnlyDataset(y_train, train_indices)
    test_dataset = LLMOnlyDataset(y_test, test_indices)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    logger.info(f"\nâœ“ æ•°æ®é›†æ„å»ºå®Œæˆ")
    logger.info(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    logger.info(f"  æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # ==================== 4. æ¨¡å‹å®šä¹‰ ====================
    class LLMOnly_Classification(nn.Module):
        def __init__(self, num_classes, llm_hidden_size=768, dropout=0.3):
            super(LLMOnly_Classification, self).__init__()
            self.d_model = llm_hidden_size  # 768
            
            logger.info("  æ„å»ºLLM-Onlyåˆ†ç±»å™¨ï¼ˆç›´æ¥ä½¿ç”¨Prompt Embeddingsï¼‰")
            
            # ğŸ”¥ ç§»é™¤RoBERTaå±‚ï¼Œç›´æ¥åœ¨pooled embeddingsä¸Šåˆ†ç±»
            # åŸå› ï¼špooled_last_tokenå·²ç»æ˜¯RoBERTaç¼–ç +æ± åŒ–çš„ç»“æœ
            # å†è¾“å…¥RoBERTa(å•token)ä¼šé€€åŒ–ä¸ºæ’ç­‰æ˜ å°„
            
            # æŠ•å½±å±‚ï¼šæå–åˆ¤åˆ«ç‰¹å¾
            self.feature_extractor = nn.Sequential(
                nn.Linear(self.d_model, self.d_model),
                nn.LayerNorm(self.d_model),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(self.d_model, self.d_model // 2),
                nn.LayerNorm(self.d_model // 2),
                nn.GELU(),
                nn.Dropout(dropout)
            )
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Linear(self.d_model // 2, num_classes)
            
            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
            logger.info(f"  âœ“ åˆ†ç±»å™¨æ„å»ºå®Œæˆï¼ˆå¯è®­ç»ƒå‚æ•°ï¼š{trainable_params:,}ï¼‰")
            
            self._initialize_weights()
        
        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        
        def forward(self, x_llm_prompt):
            """
            ç›´æ¥ä½¿ç”¨Prompt Embeddingsè¿›è¡Œåˆ†ç±»
            
            Args:
                x_llm_prompt: (batch, 1, 768) ç¦»çº¿prompt embeddings
            """
            # 1. ç¡®ä¿ç»´åº¦ (batch, 1, 768) -> (batch, 768)
            if x_llm_prompt.dim() == 3:
                x_llm_prompt = x_llm_prompt.squeeze(1)
            
            # 2. ç‰¹å¾æå–
            features = self.feature_extractor(x_llm_prompt)  # (batch, 384)
            
            # 3. åˆ†ç±»
            logits = self.classifier(features)  # (batch, num_classes)
            
            return logits
    
    model = LLMOnly_Classification(
        num_classes=num_classes,
        dropout=DROPOUT
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"\nâœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    # ==================== 5. è®­ç»ƒ ====================
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    
    criterion = nn.CrossEntropyLoss()
    # å¯¹é½ProLLMï¼šä½¿ç”¨StepLR
    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=50, gamma=0.8
    )
    
    logger.info("\n" + "="*60)
    logger.info("å¼€å§‹è®­ç»ƒï¼ˆLLM-Onlyåˆ†ç±»ä»»åŠ¡ï¼‰")
    logger.info(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    logger.info(f"  æ—©åœç­–ç•¥: å‡†ç¡®ç‡>{EARLY_STOP_THRESHOLD:.3f}ä¸”è¿ç»­{EARLY_STOP_PATIENCE}è½®ä¸æå‡")
    logger.info("="*60)
    
    best_test_acc = 0
    no_improve_count = 0  # æ—©åœè®¡æ•°å™¨
    training_start_time = time.time()
    
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_preds, train_trues = [], []
        
        for y_batch, indices_batch in train_loader:
            y_batch = y_batch.to(device)
            
            # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddingsï¼ˆè¶…å¿«ï¼ï¼‰
            embeddings = train_embeddings[indices_batch]  # (batch, 1, d_model)
            
            optimizer.zero_grad()
            logits = model(embeddings)
            loss = criterion(logits, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.extend(logits.argmax(dim=1).cpu().numpy())
            train_trues.extend(y_batch.cpu().numpy())
        
        train_loss /= len(train_loader)
        train_acc = accuracy_score(train_trues, train_preds)
        
        # æµ‹è¯•
        model.eval()
        test_preds, test_trues = [], []
        
        with torch.no_grad():
            for y_batch, indices_batch in test_loader:
                y_batch = y_batch.to(device)
                
                # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddings
                embeddings = test_embeddings[indices_batch]
                
                logits = model(embeddings)
                test_preds.extend(logits.argmax(dim=1).cpu().numpy())
                test_trues.extend(y_batch.cpu().numpy())
        
        test_acc = accuracy_score(test_trues, test_preds)
        test_f1 = f1_score(test_trues, test_preds, average='weighted')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ & æ—©åœé€»è¾‘
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            save_status = "âœ“"
            no_improve_count = 0  # é‡ç½®è®¡æ•°å™¨
        else:
            save_status = ""
            # åªæœ‰è¾¾åˆ°é˜ˆå€¼åæ‰å¼€å§‹è®¡æ•°
            if test_acc >= EARLY_STOP_THRESHOLD:
                no_improve_count += 1
        
        # è®¡ç®—epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        
        logger.info(
            f"Epoch {epoch+1:3d}/{EPOCHS} | "
            f"TrLoss: {train_loss:.4f} | TrAcc: {train_acc:.4f} | "
            f"TeAcc: {test_acc:.4f} | F1: {test_f1:.4f} | "
            f"Time: {epoch_time:.2f}s | {save_status}"
        )
        
        # ğŸ¯ æ—©åœæ£€æŸ¥1: è¾¾åˆ°å®Œç¾å‡†ç¡®ç‡ï¼Œç›´æ¥åœæ­¢
        if test_acc >= 1.0:
            logger.info(f"\nğŸ‰ å®Œç¾å‡†ç¡®ç‡è¾¾æˆï¼")
            logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f} (100%)")
            logger.info(f"  æ— éœ€ç»§ç»­è®­ç»ƒï¼Œæå‰ç»“æŸ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        # ğŸ¯ æ—©åœæ£€æŸ¥2: é«˜å‡†ç¡®ç‡ä½†è¿ç»­ä¸æå‡
        if test_acc >= EARLY_STOP_THRESHOLD and no_improve_count >= EARLY_STOP_PATIENCE:
            logger.info(f"\nâš¡ æ—©åœè§¦å‘ï¼")
            logger.info(f"  å½“å‰å‡†ç¡®ç‡: {test_acc:.4f} (>{EARLY_STOP_THRESHOLD:.3f})")
            logger.info(f"  è¿ç»­ {no_improve_count} ä¸ªepochæ— æå‡")
            logger.info(f"  æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}")
            logger.info(f"  æå‰ç»“æŸè®­ç»ƒ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        # å­¦ä¹ ç‡è¡°å‡ï¼ˆå¯¹é½ProLLMï¼šåœ¨epochç»“æŸæ—¶è°ƒç”¨ï¼‰
        scheduler.step()
    
    # ==================== 6. æœ€ç»ˆè¯„ä¼° ====================
    total_training_time = time.time() - training_start_time
    logger.info(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_training_time:.2f}s ({total_training_time/60:.2f}min)")
    
    logger.info("\n" + "="*60)
    logger.info("æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°")
    logger.info("="*60)
    
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    model.eval()
    
    test_preds, test_trues = [], []
    with torch.no_grad():
        for y_batch, indices_batch in test_loader:
            # ğŸš€ ç›´æ¥ç´¢å¼•é¢„åŠ è½½çš„embeddings
            embeddings = test_embeddings[indices_batch]
            logits = model(embeddings)
            test_preds.extend(logits.argmax(dim=1).cpu().numpy())
            test_trues.extend(y_batch.cpu().numpy())
    
    test_acc = accuracy_score(test_trues, test_preds)
    test_f1 = f1_score(test_trues, test_preds, average='weighted')

    logger.info(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½ï¼ˆLLM-Onlyï¼‰")
    logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"  æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"\n{classification_report(test_trues, test_preds)}")
    logger.info(f"\nâœ“ æ¨¡å‹ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
    
    print(f"\nâœ… æ•°æ®é›† {DATASET_NAME} è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}\n")

print("\n" + "="*80)
print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
print("="*80)
