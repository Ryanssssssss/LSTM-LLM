"""
çº¯Transformeråˆ†ç±»æ¨¡å‹ - BenchmarkåŸºå‡†æ¨¡å‹
ä½¿ç”¨æ ‡å‡†Transformer Encoderè¿›è¡Œæ—¶åºåˆ†ç±»
"""
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
import warnings
import logging
import os
import time
import math
from datetime import datetime

warnings.filterwarnings('ignore')

# ==================== âš™ï¸ è¶…å‚æ•°é…ç½® ====================
import argparse

# å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--dataset", type=str, default=None, help="æ•°æ®é›†åç§°ï¼Œä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰con*Sensor")
parser.add_argument("--device", type=str, default="cuda", choices=["cpu", "cuda"], help="è®¡ç®—è®¾å¤‡")
args = parser.parse_args()

DATA_DIR = "ProLLM/con_normalized"

# Transformerå‚æ•°
D_MODEL = 128  # æ¨¡å‹ç»´åº¦
N_HEADS = 8    # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¿…é¡»èƒ½æ•´é™¤d_modelï¼‰
N_LAYERS = 4   # Transformerå±‚æ•°
DIM_FEEDFORWARD = 512  # FFNç»´åº¦
DROPOUT = 0.2

BATCH_SIZE = 4
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4 
EPOCHS = 100
GRAD_CLIP_NORM = 1.0

# æ—©åœé…ç½®
EARLY_STOP_PATIENCE = 15
EARLY_STOP_THRESHOLD = 0.995

RANDOM_SEED = 42
# ==================== âš™ï¸ é…ç½®ç»“æŸ ====================

os.makedirs('logs', exist_ok=True)
os.makedirs('checkpoints', exist_ok=True)

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True

set_seed(RANDOM_SEED)
device = torch.device(args.device if torch.cuda.is_available() else 'cpu')

# ==================== è·å–æ•°æ®é›†åˆ—è¡¨ ====================
SOURCE_CONS = range(1, 7)
TARGET_CONS = range(1, 7)

if args.dataset:
    datasets = [args.dataset]
else:
    datasets = []
    for src in SOURCE_CONS:
        for tgt in TARGET_CONS:
            datasets.append(f"con{src}con{tgt}Sensor")
    
    print(f"\nğŸ¯ è®­ç»ƒèŒƒå›´: {len(datasets)} ä¸ªæ•°æ®é›†")
    print(f"  æºæµ“åº¦: con{min(SOURCE_CONS)}-con{max(SOURCE_CONS)}")
    print(f"  ç›®æ ‡æµ“åº¦: con{min(TARGET_CONS)}-con{max(TARGET_CONS)}")
    print()

# ==================== ä½ç½®ç¼–ç  ====================
class PositionalEncoding(nn.Module):
    """æ ‡å‡†æ­£å¼¦ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

# ==================== è®­ç»ƒæ¯ä¸ªæ•°æ®é›† ====================
for DATASET_NAME in datasets:
    print("\n" + "="*80)
    print(f"å¼€å§‹è®­ç»ƒæ•°æ®é›†: {DATASET_NAME}")
    print("="*80)
    
    MODEL_SAVE_PATH = f"checkpoints/best_transformer_{DATASET_NAME}.pth"
    log_filename = f"logs/{DATASET_NAME}_transformer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # é‡æ–°é…ç½®logger
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    logger = logging.getLogger(__name__)
    
    logger.info("="*60)
    logger.info(f"âš¡ çº¯Transformeråˆ†ç±»æ¨¡å‹ï¼ˆBenchmarkï¼‰- {DATASET_NAME}")
    logger.info("="*60)
    logger.info(f"è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    train_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_x.npy"
    train_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_train_y.npy"
    test_x_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_x.npy"
    test_y_path = f"{DATA_DIR}/{DATASET_NAME}/{DATASET_NAME}_test_y.npy"
    
    X_train = np.load(train_x_path)  # (N, channels, length)
    y_train = np.load(train_y_path)
    X_test = np.load(test_x_path)
    y_test = np.load(test_y_path)
    
    # å°†æ ‡ç­¾æ˜ å°„åˆ°0-basedç´¢å¼•
    y_train = y_train - 1
    y_test = y_test - 1
    
    num_classes = len(np.unique(y_train))
    
    logger.info(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    logger.info(f"  è®­ç»ƒé›†: {X_train.shape}")
    logger.info(f"  æµ‹è¯•é›†: {X_test.shape}")
    logger.info(f"  ç±»åˆ«æ•°: {num_classes}")
    logger.info(f"  åºåˆ—é•¿åº¦: {X_train.shape[2]}")
    
    # Dataset
    class SimpleDataset(Dataset):
        def __init__(self, X, y):
            # X: (N, channels, length) -> (N, length, channels)
            self.X = torch.FloatTensor(X).permute(0, 2, 1)
            self.y = torch.LongTensor(y)
        
        def __len__(self):
            return len(self.X)
        
        def __getitem__(self, idx):
            return self.X[idx], self.y[idx]
    
    train_dataset = SimpleDataset(X_train, y_train)
    test_dataset = SimpleDataset(X_test, y_test)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    # Transformeræ¨¡å‹
    class Transformer_Classification(nn.Module):
        def __init__(self, input_size, d_model, n_heads, n_layers, 
                     dim_feedforward, num_classes, dropout=0.3):
            super(Transformer_Classification, self).__init__()
            
            # è¾“å…¥æŠ•å½±å±‚ï¼ˆå°†input_sizeæŠ•å½±åˆ°d_modelï¼‰
            self.input_projection = nn.Linear(input_size, d_model)
            
            # ä½ç½®ç¼–ç 
            self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)
            
            # Transformer Encoder
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=n_heads,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                activation='gelu',
                batch_first=True  # ğŸ”¥ é‡è¦ï¼šä½¿ç”¨batch_first
            )
            self.transformer_encoder = nn.TransformerEncoder(
                encoder_layer, 
                num_layers=n_layers
            )
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Linear(d_model, 256),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(256, 128),
                nn.GELU(),
                nn.Dropout(dropout * 0.5),
                nn.Linear(128, num_classes)
            )
        
        def forward(self, x):
            # x: (batch, seq_len, input_size)
            
            # 1. æŠ•å½±åˆ°d_modelç»´åº¦
            x = self.input_projection(x)  # (batch, seq_len, d_model)
            
            # 2. æ·»åŠ ä½ç½®ç¼–ç 
            x = self.pos_encoder(x)
            
            # 3. Transformerç¼–ç 
            x = self.transformer_encoder(x)  # (batch, seq_len, d_model)
            
            # 4. å…¨å±€å¹³å‡æ± åŒ–ï¼ˆæˆ–å–æœ€åä¸€ä¸ªtokenï¼‰
            x = x.mean(dim=1)  # (batch, d_model)
            # æˆ–è€…å–æœ€åä¸€ä¸ª: x = x[:, -1, :]
            
            # 5. åˆ†ç±»
            logits = self.classifier(x)
            return logits
    
    model = Transformer_Classification(
        input_size=X_train.shape[1],  # channels
        d_model=D_MODEL,
        n_heads=N_HEADS,
        n_layers=N_LAYERS,
        dim_feedforward=DIM_FEEDFORWARD,
        num_classes=num_classes,
        dropout=DROPOUT
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"\nâœ“ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  æ€»å‚æ•°é‡: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"  æ¨¡å‹ç»´åº¦: d_model={D_MODEL}, n_heads={N_HEADS}, n_layers={N_LAYERS}")
    
    # è®­ç»ƒ
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    criterion = nn.CrossEntropyLoss()
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)
    
    logger.info("\n" + "="*60)
    logger.info("å¼€å§‹è®­ç»ƒï¼ˆTransformer Benchmarkï¼‰")
    logger.info(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    logger.info(f"  æ—©åœç­–ç•¥: å‡†ç¡®ç‡>{EARLY_STOP_THRESHOLD:.3f}ä¸”è¿ç»­{EARLY_STOP_PATIENCE}è½®ä¸æå‡")
    logger.info("="*60)
    
    best_test_acc = 0
    no_improve_count = 0
    training_start_time = time.time()
    
    for epoch in range(EPOCHS):
        epoch_start_time = time.time()
        model.train()
        train_loss = 0
        train_preds, train_trues = [], []
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            logits = model(X_batch)
            loss = criterion(logits, y_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_NORM)
            optimizer.step()
            train_loss += loss.item()
            train_preds.extend(logits.argmax(dim=1).cpu().numpy())
            train_trues.extend(y_batch.cpu().numpy())
        
        train_loss /= len(train_loader)
        train_acc = accuracy_score(train_trues, train_preds)
        
        # æµ‹è¯•
        model.eval()
        test_preds, test_trues = [], []
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch = X_batch.to(device)
                logits = model(X_batch)
                test_preds.extend(logits.argmax(dim=1).cpu().numpy())
                test_trues.extend(y_batch.cpu().numpy())
        
        test_acc = accuracy_score(test_trues, test_preds)
        test_f1 = f1_score(test_trues, test_preds, average='weighted')
        
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), MODEL_SAVE_PATH)
            save_status = "âœ“"
            no_improve_count = 0
        else:
            save_status = ""
            if test_acc >= EARLY_STOP_THRESHOLD:
                no_improve_count += 1
        
        epoch_time = time.time() - epoch_start_time
        
        logger.info(
            f"Epoch {epoch+1:3d}/{EPOCHS} | "
            f"TrLoss: {train_loss:.4f} | TrAcc: {train_acc:.4f} | "
            f"TeAcc: {test_acc:.4f} | F1: {test_f1:.4f} | "
            f"Time: {epoch_time:.2f}s | {save_status}"
        )
        
        # æ—©åœæ£€æŸ¥
        if test_acc >= 1.0:
            logger.info(f"\nğŸ‰ å®Œç¾å‡†ç¡®ç‡è¾¾æˆï¼æå‰ç»“æŸ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        if test_acc >= EARLY_STOP_THRESHOLD and no_improve_count >= EARLY_STOP_PATIENCE:
            logger.info(f"\nâš¡ æ—©åœè§¦å‘ï¼è¿ç»­ {no_improve_count} ä¸ªepochæ— æå‡")
            logger.info(f"  æå‰ç»“æŸè®­ç»ƒ (Epoch {epoch+1}/{EPOCHS})")
            break
        
        scheduler.step()
    
    # æœ€ç»ˆè¯„ä¼°
    total_training_time = time.time() - training_start_time
    logger.info(f"\nâœ“ è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {total_training_time:.2f}s ({total_training_time/60:.2f}min)")
    
    model.load_state_dict(torch.load(MODEL_SAVE_PATH))
    model.eval()
    
    test_preds, test_trues = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            logits = model(X_batch)
            test_preds.extend(logits.argmax(dim=1).cpu().numpy())
            test_trues.extend(y_batch.cpu().numpy())
    
    test_acc = accuracy_score(test_trues, test_preds)
    test_f1 = f1_score(test_trues, test_preds, average='weighted')
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š æœ€ç»ˆæ€§èƒ½ï¼ˆTransformer Benchmarkï¼‰")
    logger.info("="*60)
    logger.info(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.info(f"  æµ‹è¯•F1: {test_f1:.4f}")
    logger.info(f"\n{classification_report(test_trues, test_preds)}")
    
    print(f"\nâœ… æ•°æ®é›† {DATASET_NAME} è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_test_acc:.4f}\n")

print("\n" + "="*80)
print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
print("="*80)
