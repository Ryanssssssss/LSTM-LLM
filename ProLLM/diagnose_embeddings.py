#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯Šæ–­è„šæœ¬ï¼šæ£€æŸ¥ offline embedding æ•°æ®ä¸­çš„ NaN é—®é¢˜
ç‰¹åˆ«å…³æ³¨ batch 81 å’Œç›¸å…³ç´¢å¼•
"""

import os
import h5py
import numpy as np
import torch
from pathlib import Path
import argparse

def check_single_embedding(file_path, idx, representation='sequence'):
    """æ£€æŸ¥å•ä¸ª embedding æ–‡ä»¶"""
    issues = []
    
    try:
        with h5py.File(file_path, 'r') as hf:
            if 'embeddings' not in hf:
                issues.append(f"âŒ ç¼ºå°‘ 'embeddings' æ•°æ®é›†")
                return issues, None
            
            data = hf['embeddings'][:]
            
            # åŸºæœ¬ä¿¡æ¯
            print(f"  ç´¢å¼• {idx}: å½¢çŠ¶={data.shape}, dtype={data.dtype}")
            
            # æ£€æŸ¥ NaN
            nan_count = np.isnan(data).sum()
            if nan_count > 0:
                issues.append(f"ğŸ”´ åŒ…å« {nan_count} ä¸ª NaN å€¼ (æ€»å…± {data.size} ä¸ªå…ƒç´ , å æ¯” {nan_count/data.size*100:.2f}%)")
            
            # æ£€æŸ¥ Inf
            inf_count = np.isinf(data).sum()
            if inf_count > 0:
                issues.append(f"ğŸ”´ åŒ…å« {inf_count} ä¸ª Inf å€¼")
            
            # æ£€æŸ¥æç«¯å€¼
            if nan_count == 0 and inf_count == 0:
                data_min, data_max = data.min(), data.max()
                data_mean, data_std = data.mean(), data.std()
                print(f"    å€¼èŒƒå›´: [{data_min:.4f}, {data_max:.4f}]")
                print(f"    å‡å€¼/æ ‡å‡†å·®: {data_mean:.4f} / {data_std:.4f}")
                
                # æ£€æŸ¥å¼‚å¸¸æç«¯å€¼
                if abs(data_max) > 1000 or abs(data_min) > 1000:
                    issues.append(f"âš ï¸  å­˜åœ¨æç«¯å€¼: min={data_min:.2f}, max={data_max:.2f}")
            
            return issues, data
            
    except Exception as e:
        issues.append(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        return issues, None

def diagnose_dataset_embeddings(dataset_name, representation='sequence', batch_size=32):
    """è¯Šæ–­æŒ‡å®šæ•°æ®é›†çš„ embeddings"""
    print(f"\n{'='*80}")
    print(f"è¯Šæ–­æ•°æ®é›†: {dataset_name}")
    print(f"è¡¨ç¤ºæ–¹å¼: {representation}")
    print(f"{'='*80}")
    
    base_dir = f"embeddings/{dataset_name}"
    if representation != 'sequence':
        base_dir = f"{base_dir}/{representation}"
    
    splits = ['train', 'test']
    all_issues = {}
    
    for split in splits:
        print(f"\nğŸ“‚ æ£€æŸ¥ {split} é›†:")
        split_dir = os.path.join(base_dir, split)
        
        if not os.path.exists(split_dir):
            print(f"  âŒ ç›®å½•ä¸å­˜åœ¨: {split_dir}")
            continue
        
        # è·å–æ‰€æœ‰ embedding æ–‡ä»¶
        files = sorted([f for f in os.listdir(split_dir) if f.endswith('.h5')], 
                      key=lambda x: int(x.replace('.h5', '')))
        
        print(f"  æ‰¾åˆ° {len(files)} ä¸ª embedding æ–‡ä»¶")
        
        if len(files) == 0:
            continue
        
        # æ£€æŸ¥ç‰¹å®šç´¢å¼•ï¼ˆbatch 81 ç›¸å…³ï¼‰
        # batch_size=32, batch_81 åŒ…å«ç´¢å¼• 2592-2623
        batch_81_start = 81 * batch_size
        batch_81_end = batch_81_start + batch_size
        
        print(f"\n  ğŸ¯ é‡ç‚¹æ£€æŸ¥ Batch 81 (ç´¢å¼• {batch_81_start}-{batch_81_end-1}):")
        
        batch_81_issues = []
        for idx in range(batch_81_start, min(batch_81_end, len(files))):
            file_path = os.path.join(split_dir, f"{idx}.h5")
            if os.path.exists(file_path):
                issues, data = check_single_embedding(file_path, idx, representation)
                if issues:
                    batch_81_issues.append((idx, issues))
                    print(f"    {'  '.join(issues)}")
        
        # éšæœºæŠ½æŸ¥å…¶ä»–æ ·æœ¬
        print(f"\n  ğŸ“Š éšæœºæŠ½æŸ¥å…¶ä»–æ ·æœ¬:")
        sample_indices = np.random.choice(len(files), min(10, len(files)), replace=False)
        sample_indices = [idx for idx in sample_indices if idx < batch_81_start or idx >= batch_81_end]
        
        other_issues = []
        for idx in sorted(sample_indices):
            file_path = os.path.join(split_dir, f"{idx}.h5")
            if os.path.exists(file_path):
                issues, data = check_single_embedding(file_path, idx, representation)
                if issues:
                    other_issues.append((idx, issues))
                    print(f"    {'  '.join(issues)}")
        
        # ç»Ÿè®¡æ±‡æ€»
        print(f"\n  ğŸ“ˆ {split} é›†ç»Ÿè®¡:")
        print(f"    Batch 81 é—®é¢˜æ•°: {len(batch_81_issues)}")
        print(f"    å…¶ä»–æ ·æœ¬é—®é¢˜æ•°: {len(other_issues)} / {len(sample_indices)} (æŠ½æ ·)")
        
        all_issues[split] = {
            'batch_81': batch_81_issues,
            'others': other_issues
        }
    
    return all_issues

def check_raw_data(dataset_name, batch_size=32):
    """æ£€æŸ¥åŸå§‹è¾“å…¥æ•°æ®æ˜¯å¦æœ‰é—®é¢˜"""
    print(f"\n{'='*80}")
    print(f"æ£€æŸ¥åŸå§‹æ•°æ®: {dataset_name}")
    print(f"{'='*80}")
    
    train_path = f"npydata/{dataset_name}/{dataset_name}_train_x.npy"
    test_path = f"npydata/{dataset_name}/{dataset_name}_test_x.npy"
    
    for split, path in [('train', train_path), ('test', test_path)]:
        print(f"\nğŸ“‚ {split} é›†:")
        
        if not os.path.exists(path):
            print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            continue
        
        data = np.load(path)
        print(f"  å½¢çŠ¶: {data.shape}")
        print(f"  dtype: {data.dtype}")
        
        # æ£€æŸ¥ NaN/Inf
        nan_count = np.isnan(data).sum()
        inf_count = np.isinf(data).sum()
        
        print(f"  NaN æ•°é‡: {nan_count}")
        print(f"  Inf æ•°é‡: {inf_count}")
        
        if nan_count == 0 and inf_count == 0:
            print(f"  å€¼èŒƒå›´: [{data.min():.4f}, {data.max():.4f}]")
            print(f"  å‡å€¼/æ ‡å‡†å·®: {data.mean():.4f} / {data.std():.4f}")
        
        # æ£€æŸ¥ batch 81
        batch_81_start = 81 * batch_size
        batch_81_end = min(batch_81_start + batch_size, len(data))
        
        if batch_81_start < len(data):
            print(f"\n  ğŸ¯ Batch 81 æ•°æ® (ç´¢å¼• {batch_81_start}-{batch_81_end-1}):")
            batch_data = data[batch_81_start:batch_81_end]
            batch_nan = np.isnan(batch_data).sum()
            batch_inf = np.isinf(batch_data).sum()
            
            print(f"    å½¢çŠ¶: {batch_data.shape}")
            print(f"    NaN æ•°é‡: {batch_nan}")
            print(f"    Inf æ•°é‡: {batch_inf}")
            
            if batch_nan == 0 and batch_inf == 0:
                print(f"    å€¼èŒƒå›´: [{batch_data.min():.4f}, {batch_data.max():.4f}]")
                print(f"    å‡å€¼/æ ‡å‡†å·®: {batch_data.mean():.4f} / {batch_data.std():.4f}")
            else:
                print(f"    ğŸ”´ åŸå§‹æ•°æ®ä¸­ Batch 81 å°±æœ‰é—®é¢˜!")

def main():
    parser = argparse.ArgumentParser(description="è¯Šæ–­ offline embedding æ•°æ®")
    parser.add_argument('--dataset', type=str, default='Sensor', help='æ•°æ®é›†åç§°')
    parser.add_argument('--representation', type=str, default='pooled_last_token', 
                       choices=['sequence', 'pooled_last_token'],
                       help='Embedding è¡¨ç¤ºæ–¹å¼')
    parser.add_argument('--batch_size', type=int, default=32, help='è®­ç»ƒæ—¶ä½¿ç”¨çš„ batch size')
    parser.add_argument('--check_raw', action='store_true', help='æ˜¯å¦æ£€æŸ¥åŸå§‹æ•°æ®')
    
    args = parser.parse_args()
    
    print(f"\nğŸ” Offline Embedding è¯Šæ–­å·¥å…·")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"è¡¨ç¤ºæ–¹å¼: {args.representation}")
    print(f"Batch Size: {args.batch_size}")
    
    # æ£€æŸ¥åŸå§‹æ•°æ®
    if args.check_raw:
        check_raw_data(args.dataset, args.batch_size)
    
    # æ£€æŸ¥ embeddings
    issues = diagnose_dataset_embeddings(args.dataset, args.representation, args.batch_size)
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"ğŸ“ è¯Šæ–­æŠ¥å‘Šæ±‡æ€»")
    print(f"{'='*80}")
    
    total_issues = 0
    for split, split_issues in issues.items():
        batch_81_count = len(split_issues['batch_81'])
        others_count = len(split_issues['others'])
        total = batch_81_count + others_count
        total_issues += total
        
        print(f"\n{split} é›†:")
        print(f"  Batch 81 é—®é¢˜: {batch_81_count}")
        print(f"  å…¶ä»–é—®é¢˜: {others_count}")
        print(f"  æ€»è®¡: {total}")
        
        if batch_81_count > 0:
            print(f"\n  ğŸ”´ Batch 81 æœ‰é—®é¢˜çš„ç´¢å¼•:")
            for idx, issue_list in split_issues['batch_81']:
                print(f"    - ç´¢å¼• {idx}: {issue_list[0]}")
    
    if total_issues == 0:
        print(f"\nâœ… æœªå‘ç°é—®é¢˜!")
    else:
        print(f"\nâš ï¸  å‘ç° {total_issues} ä¸ªé—®é¢˜ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ embeddings!")
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"  1. æ£€æŸ¥åŸå§‹æ•°æ®æ˜¯å¦æœ‰ NaN/Inf")
        print(f"  2. æ£€æŸ¥ prompt_handler.py ä¸­çš„ generate_prompt() æ˜¯å¦äº§ç”Ÿå¼‚å¸¸")
        print(f"  3. é‡æ–°è¿è¡Œ offline_embedding_generator.py ç”Ÿæˆ embeddings")

if __name__ == "__main__":
    main()
