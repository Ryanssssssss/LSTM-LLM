{
    "gpt2": {
        "dim": 768,
        "path": "/home/wuwujian/LXY/sensor_process/LLM-few/gpt2_small",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["c_attn", "c_proj"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "gpt2-medium": {
        "dim": 1024,
        "path": "/path/to/your/pretrained/folder/gpt2-medium",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["c_attn"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "gpt2-large": {
        "dim": 1280,
        "path": "/path/to/your/pretrained/folder/gpt2-large",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["c_attn"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "llama3": {
        "dim": 4096,
        "path": "/path/to/your/pretrained/folder/Meta-Llama-3-8B",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["_proj"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "qwen0.5": {
        "dim": 896,
        "path": "/path/to/your/pretrained/folder/Qwen2-0.5B-Instruct",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "k_proj", "v_proj"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "qwen1.5": {
        "dim": 1536,
        "path": "/path/to/your/pretrained/folder/Qwen2-1.5B-Instruct",
        "model_class": "AutoModel",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "k_proj", "v_proj"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    },
    "phi3": {
        "dim": 3072,
        "path": "/path/to/your/pretrained/folder/Phi-3-mini-4k-instruct",
        "model_class": "Phi3Model",
        "lora_config": {
            "r": 8,
            "lora_alpha": 32,
            "target_modules": ["qkv_proj"],
            "lora_dropout": 0.1,
            "bias": "none"
        }
    }
}
