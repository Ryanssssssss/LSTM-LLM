"""
ä¸ºETDataseté¢„ç”ŸæˆGPT-2 Embeddingsï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
åªéœ€è¿è¡Œä¸€æ¬¡ï¼Œåç»­è®­ç»ƒç›´æ¥åŠ è½½
"""
import numpy as np
import pandas as pd
import torch
from transformers import GPT2Model, GPT2Tokenizer
from sklearn.preprocessing import StandardScaler
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½® ====================
LOOK_BACK = 96
N_FUTURE = 24
TRAIN_RATIO = 0.6
VAL_RATIO = 0.2
TEST_RATIO = 0.2

DATA_FILE = "ETDataset/ETT-small/ETTh1.csv"
SAVE_DIR = "embeddings/ett"
# ==================== é…ç½®ç»“æŸ ====================

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs(f"{SAVE_DIR}/train", exist_ok=True)
os.makedirs(f"{SAVE_DIR}/val", exist_ok=True)
os.makedirs(f"{SAVE_DIR}/test", exist_ok=True)

# ==================== 1. æ•°æ®åŠ è½½ ====================
print("\n" + "="*60)
print("1. åŠ è½½ETDatasetæ•°æ®")
print("="*60)

df = pd.read_csv(DATA_FILE)
df['date'] = pd.to_datetime(df['date'])
print(f"âœ“ æ•°æ®åŠ è½½: {df.shape[0]}æ¡è®°å½•")

feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']
target_col = 'OT'
data = df[feature_cols + [target_col]].values

# æ ‡å‡†åŒ–
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
print(f"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")

# ä¿å­˜scalerï¼ˆè®­ç»ƒæ—¶éœ€è¦åæ ‡å‡†åŒ–ï¼‰
np.save(f"{SAVE_DIR}/scaler_mean.npy", scaler.mean_)
np.save(f"{SAVE_DIR}/scaler_scale.npy", scaler.scale_)
print(f"âœ“ Scalerå·²ä¿å­˜è‡³ {SAVE_DIR}/")

# ==================== 2. æ„å»ºæ—¶åºåºåˆ— ====================
def create_sequences(data, look_back, n_future):
    X, y = [], []
    for i in range(len(data) - look_back - n_future + 1):
        X.append(data[i:i+look_back, :])
        y.append(data[i+look_back:i+look_back+n_future, -1])
    return np.array(X), np.array(y)

X, y = create_sequences(data_scaled, LOOK_BACK, N_FUTURE)
print(f"\nâœ“ æ—¶åºåºåˆ—æ„å»ºå®Œæˆ: X{X.shape}, y{y.shape}")

# æ•°æ®åˆ’åˆ†
train_size = int(len(X) * TRAIN_RATIO)
val_size = int(len(X) * VAL_RATIO)

X_train = X[:train_size]
X_val = X[train_size:train_size+val_size]
X_test = X[train_size+val_size:]

print(f"âœ“ æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ{len(X_train)} | éªŒè¯{len(X_val)} | æµ‹è¯•{len(X_test)}")

# ==================== 3. å®šä¹‰Promptç”Ÿæˆå‡½æ•° ====================
def generate_electricity_prompt(sequence):
    """
    åŸºäºæ¨¡å¼è¯†åˆ«çš„é¢†åŸŸçŸ¥è¯†æ£€ç´¢
    æ ¸å¿ƒæ€æƒ³ï¼šè¯†åˆ«å½“å‰æ ·æœ¬çš„è¿è¡Œæ¨¡å¼ï¼Œç„¶åæ£€ç´¢å¯¹åº”çš„ä¸“å®¶ç»éªŒå’Œç‰©ç†è§„å¾‹
    """
    hufl = sequence[:, 0]  # é«˜å‹ä¾§æœ‰åŠŸè´Ÿè·
    hull = sequence[:, 1]  # é«˜å‹ä¾§æ— åŠŸè´Ÿè·
    mufl = sequence[:, 2]  # ä¸­å‹ä¾§æœ‰åŠŸè´Ÿè·
    mull = sequence[:, 3]  # ä¸­å‹ä¾§æ— åŠŸè´Ÿè·
    lufl = sequence[:, 4]  # ä½å‹ä¾§æœ‰åŠŸè´Ÿè·
    lull = sequence[:, 5]  # ä½å‹ä¾§æ— åŠŸè´Ÿè·
    ot = sequence[:, 6]    # æ²¹æ¸©
    
    # ========== æ¨¡å¼è¯†åˆ«ï¼ˆé«˜å±‚æ¬¡ç‰¹å¾ï¼ŒLSTMéš¾ä»¥ç›´æ¥å­¦ä¹ ï¼‰==========
    
    # 1. è´Ÿè·æ³¢åŠ¨æ¨¡å¼è¯†åˆ«
    total_load = hufl + mufl + lufl
    load_std = total_load.std()
    load_cv = load_std / (total_load.mean() + 1e-8)  # å˜å¼‚ç³»æ•°
    
    if load_cv > 0.5:
        volatility_pattern = "å‰§çƒˆæ³¢åŠ¨å‹"
        volatility_knowledge = "å‰§çƒˆæ³¢åŠ¨å·¥å†µä¸‹ï¼Œæ²¹æ¸©å“åº”å‘ˆç°éçº¿æ€§ç‰¹å¾ï¼Œéœ€å…³æ³¨å³°å€¼è´Ÿè·çš„ç´¯ç§¯çƒ­æ•ˆåº”å’Œå¿«é€Ÿæ•£çƒ­èƒ½åŠ›ã€‚"
    elif load_cv > 0.25:
        volatility_pattern = "ä¸­åº¦æ³¢åŠ¨å‹"
        volatility_knowledge = "ä¸­åº¦æ³¢åŠ¨å·¥å†µå±äºå…¸å‹åŸå¸‚è´Ÿè·æ¨¡å¼ï¼Œæ²¹æ¸©å˜åŒ–å…·æœ‰æ˜æ˜¾æ»åæ€§ï¼ˆ2-3å°æ—¶ï¼‰ï¼Œå¯å‚è€ƒçƒ­æƒ¯æ€§æ¨¡å‹ã€‚"
    else:
        volatility_pattern = "å¹³ç¨³å‹"
        volatility_knowledge = "å¹³ç¨³è´Ÿè·ä¸‹æ²¹æ¸©ä¸»è¦å—ç¯å¢ƒæ¸©åº¦å½±å“ï¼Œæ•£çƒ­æ•ˆç‡ç¨³å®šï¼Œé€‚åˆçº¿æ€§å¤–æ¨é¢„æµ‹ã€‚"
    
    # 2. è´Ÿè·-æ¸©åº¦è€¦åˆå¼ºåº¦åˆ†æ
    recent_load_change = total_load[-24:].mean() - total_load[-48:-24].mean()
    recent_temp_change = ot[-24:].mean() - ot[-48:-24].mean()
    
    if abs(recent_load_change) < 0.1 and abs(recent_temp_change) > 0.15:
        coupling_pattern = "æ¸©åº¦å¼‚å¸¸å‹"
        coupling_knowledge = "è´Ÿè·ç¨³å®šä½†æ¸©åº¦å¼‚å¸¸å˜åŒ–ï¼Œå¯èƒ½å­˜åœ¨æ•£çƒ­ç³»ç»Ÿæ•…éšœæˆ–ç¯å¢ƒçªå˜ï¼Œéœ€è­¦æƒ•è®¾å¤‡å¼‚å¸¸ã€‚"
    elif abs(recent_load_change) > 0.3:
        coupling_pattern = "è´Ÿè·ä¸»å¯¼å‹"
        coupling_knowledge = "è´Ÿè·å¤§å¹…å˜åŒ–æ˜¯æ¸©åº¦å˜åŒ–çš„ä¸»è¦é©±åŠ¨åŠ›ï¼Œéµå¾ªé“œæŸä¸è´Ÿè·å¹³æ–¹æˆæ­£æ¯”çš„ç‰©ç†è§„å¾‹ï¼ˆP_loss âˆ IÂ²Rï¼‰ã€‚"
    else:
        coupling_pattern = "æ­£å¸¸è€¦åˆå‹"
        coupling_knowledge = "è´Ÿè·ä¸æ¸©åº¦å‘ˆç°æ­£å¸¸è€¦åˆå…³ç³»ï¼Œç¬¦åˆæ ‡å‡†çƒ­åŠ›å­¦æ¨¡å‹ï¼Œé¢„æµ‹ç²¾åº¦ä¾èµ–äºå†å²ç›¸ä¼¼æ¨¡å¼ã€‚"
    
    # 3. å¤šçº§è´Ÿè·ååŒæ¨¡å¼
    high_ratio = hufl.mean() / (total_load.mean() + 1e-8)
    mid_ratio = mufl.mean() / (total_load.mean() + 1e-8)
    low_ratio = lufl.mean() / (total_load.mean() + 1e-8)
    
    if high_ratio > 0.6:
        load_dist_pattern = "é«˜å‹é›†ä¸­å‹"
        load_dist_knowledge = "é«˜å‹ä¾§ä¸»å¯¼ï¼ˆ>60%ï¼‰ï¼Œé“æŸå æ¯”é«˜ï¼Œæ²¹æ¸©å¯¹é«˜å‹è´Ÿè·å˜åŒ–æ•æ„Ÿåº¦çº¦ä¸ºä¸­ä½å‹çš„2-3å€ã€‚"
    elif max(high_ratio, mid_ratio, low_ratio) < 0.45:
        load_dist_pattern = "å‡è¡¡åˆ†å¸ƒå‹"
        load_dist_knowledge = "ä¸‰çº§è´Ÿè·å‡è¡¡åˆ†å¸ƒï¼Œçƒ­é‡äº§ç”Ÿè¾ƒä¸ºåˆ†æ•£ï¼Œæ•´ä½“çƒ­å¹³è¡¡ç¨³å®šæ€§å¥½ï¼Œé€‚åˆå¤šå…ƒçº¿æ€§å›å½’é¢„æµ‹ã€‚"
    else:
        load_dist_pattern = "åŒçº§ä¸»å¯¼å‹"
        load_dist_knowledge = "ä¸­é«˜å‹ååŒä¸»å¯¼ï¼Œéœ€å…³æ³¨ä¸¤çº§è´Ÿè·çš„äº¤äº’å½±å“ï¼Œå åŠ æ•ˆåº”å¯èƒ½å¯¼è‡´æ¸©å‡åŠ é€Ÿã€‚"
    
    # 4. åŠŸç‡å› æ•°ä¸æ— åŠŸå½±å“
    pf_high = abs(hufl.mean() / (hull.mean() + 1e-8))
    
    if pf_high < 1.5:
        pf_pattern = "ä½åŠŸç‡å› æ•°å‹"
        pf_knowledge = "åŠŸç‡å› æ•°ä½ï¼ˆPF<0.8ï¼‰ï¼Œæ— åŠŸç”µæµå¯¼è‡´é¢å¤–é“œæŸçº¦15-25%ï¼Œæ²¹æ¸©é¢„æµ‹éœ€ä¸Šè°ƒ5-10%ã€‚"
    elif pf_high > 3.0:
        pf_pattern = "é«˜åŠŸç‡å› æ•°å‹"
        pf_knowledge = "åŠŸç‡å› æ•°ä¼˜ç§€ï¼ˆPF>0.95ï¼‰ï¼Œè®¾å¤‡è¿è¡Œæ•ˆç‡é«˜ï¼Œæ¸©å‡ä¸»è¦æ¥è‡ªæœ‰åŠŸè´Ÿè·ï¼ŒæŸè€—è®¡ç®—å¯ç®€åŒ–ã€‚"
    else:
        pf_pattern = "æ­£å¸¸åŠŸç‡å› æ•°å‹"
        pf_knowledge = "åŠŸç‡å› æ•°æ­£å¸¸ï¼ˆ0.8<PF<0.95ï¼‰ï¼Œç¬¦åˆç”µç½‘è¿è¡Œè§„èŒƒï¼Œé‡‡ç”¨æ ‡å‡†æŸè€—æ¨¡å‹å³å¯ã€‚"
    
    # 5. æ—¶é—´æ¨¡å¼è¯†åˆ«ï¼ˆå‘¨æœŸæ€§ï¼‰
    load_autocorr_24h = np.corrcoef(total_load[:-24], total_load[24:])[0, 1]
    
    if load_autocorr_24h > 0.7:
        time_pattern = "å¼ºå‘¨æœŸå‹"
        time_knowledge = "24å°æ—¶å‘¨æœŸæ€§å¼ºï¼ˆç›¸å…³ç³»æ•°>0.7ï¼‰ï¼Œå¯åˆ©ç”¨æ˜¨æ—¥åŒæœŸæ•°æ®ï¼Œé€‚åˆARIMAç±»æ–¹æ³•ã€‚"
    elif load_autocorr_24h < 0.3:
        time_pattern = "å¼±å‘¨æœŸå‹"
        time_knowledge = "å‘¨æœŸæ€§å¼±ï¼Œå¯èƒ½ä¸ºéå·¥ä½œæ—¥æˆ–ç‰¹æ®Šäº‹ä»¶ï¼Œå†å²æ¨¡å¼å‚è€ƒä»·å€¼æœ‰é™ï¼Œéœ€ä¾èµ–å®æ—¶è¶‹åŠ¿ã€‚"
    else:
        time_pattern = "ä¸­ç­‰å‘¨æœŸå‹"
        time_knowledge = "å‘¨æœŸæ€§ä¸­ç­‰ï¼Œå»ºè®®ç»“åˆè¶‹åŠ¿åˆ†æå’Œå‘¨æœŸåˆ†è§£æ–¹æ³•ï¼ˆå¦‚STLï¼‰æé«˜é¢„æµ‹ç²¾åº¦ã€‚"
    
    # 6. æ¸©åº¦çŠ¶æ€è¯„ä¼°
    current_temp = ot[-1]
    temp_percentile = (ot < current_temp).sum() / len(ot)
    
    if temp_percentile > 0.9:
        temp_state = "é«˜æ¸©è¿è¡ŒåŒº"
        temp_knowledge = "å½“å‰æ¸©åº¦å¤„äºå†å²é«˜ä½ï¼ˆ>90åˆ†ä½ï¼‰ï¼Œæ•£çƒ­èƒ½åŠ›æ¥è¿‘é¥±å’Œï¼Œæ¸©å‡é€Ÿç‡å¯èƒ½åŠ å¿«ï¼Œæ³¨æ„85Â°CæŠ¥è­¦é˜ˆå€¼ã€‚"
    elif temp_percentile < 0.1:
        temp_state = "ä½æ¸©è¿è¡ŒåŒº"
        temp_knowledge = "å½“å‰æ¸©åº¦å¤„äºå†å²ä½ä½ï¼ˆ<10åˆ†ä½ï¼‰ï¼Œè®¾å¤‡å†·å¯åŠ¨æˆ–ä½è´Ÿè·çŠ¶æ€ï¼Œæ¸©å‡é€Ÿç‡éµå¾ªæŒ‡æ•°ä¸Šå‡è§„å¾‹ã€‚"
    else:
        temp_state = "æ­£å¸¸è¿è¡ŒåŒº"
        temp_knowledge = "æ¸©åº¦å¤„äºæ­£å¸¸åŒºé—´ï¼Œçƒ­å¹³è¡¡ç¨³å®šï¼Œé¢„æµ‹è¯¯å·®ä¸»è¦æ¥æºäºè´Ÿè·æ³¢åŠ¨å’Œç¯å¢ƒæ‰°åŠ¨ã€‚"
    
    # ========== æ„å»ºç»“æ„åŒ–Prompt ==========
    prompt = f"""<|electricity_forecasting|>ç”µåŠ›å˜å‹å™¨æ²¹æ¸©é¢„æµ‹ä»»åŠ¡ã€‚

ã€è¿è¡Œæ¨¡å¼è¯†åˆ«ã€‘
- è´Ÿè·æ³¢åŠ¨ç‰¹å¾: {volatility_pattern}
- è´Ÿè·-æ¸©åº¦è€¦åˆ: {coupling_pattern}
- å¤šçº§è´Ÿè·åˆ†å¸ƒ: {load_dist_pattern}
- åŠŸç‡å› æ•°çŠ¶æ€: {pf_pattern}
- æ—¶é—´å‘¨æœŸç‰¹å¾: {time_pattern}
- å½“å‰æ¸©åº¦çŠ¶æ€: {temp_state}

ã€é¢†åŸŸçŸ¥è¯†ä¸é¢„æµ‹ç­–ç•¥ã€‘
{volatility_knowledge}
{coupling_knowledge}
{load_dist_knowledge}
{pf_knowledge}
{time_knowledge}
{temp_knowledge}

ã€ç‰©ç†çº¦æŸã€‘
çƒ­å¹³è¡¡æ–¹ç¨‹: dT/dt = Î±Â·Î”P_loss - Î²Â·(T - T_amb)
å…¶ä¸­Î±ä¸ºçƒ­å®¹ç³»æ•°ï¼ŒÎ²ä¸ºæ•£çƒ­ç³»æ•°ï¼ŒÎ”P_lossä¸ºè´Ÿè·å˜åŒ–å¼•èµ·çš„æŸè€—å˜åŒ–ã€‚
æ»åæ—¶é—´å¸¸æ•°çº¦ä¸º2-4å°æ—¶ï¼Œå³°å€¼è´Ÿè·åéœ€æŒç»­ç›‘æµ‹3-6å°æ—¶ã€‚<|endoftext|>"""
    
    return prompt

print("\n" + "="*60)
print("3. ç”ŸæˆPromptsï¼ˆåŸºäºæ¨¡å¼è¯†åˆ«çš„é¢†åŸŸçŸ¥è¯†ï¼‰")
print("="*60)

train_prompts = [generate_electricity_prompt(seq) for seq in tqdm(X_train, desc="è®­ç»ƒé›†")]
val_prompts = [generate_electricity_prompt(seq) for seq in tqdm(X_val, desc="éªŒè¯é›†")]
test_prompts = [generate_electricity_prompt(seq) for seq in tqdm(X_test, desc="æµ‹è¯•é›†")]

print(f"\nâœ“ Promptç”Ÿæˆå®Œæˆ")
print(f"  ç¤ºä¾‹ï¼ˆå‰300å­—ç¬¦ï¼‰:\n{train_prompts[0][:300]}...")
print(f"\n  åŒ…å«çš„æ¨¡å¼è¯†åˆ«ç»´åº¦ï¼š")
print(f"    - è´Ÿè·æ³¢åŠ¨ç‰¹å¾ï¼ˆå¹³ç¨³/ä¸­åº¦/å‰§çƒˆï¼‰")
print(f"    - è´Ÿè·-æ¸©åº¦è€¦åˆæ¨¡å¼")
print(f"    - å¤šçº§è´Ÿè·åˆ†å¸ƒæ¨¡å¼")
print(f"    - åŠŸç‡å› æ•°çŠ¶æ€")
print(f"    - æ—¶é—´å‘¨æœŸæ€§")
print(f"    - æ¸©åº¦çŠ¶æ€åˆ†åŒº")

# ==================== 4. ç”ŸæˆGPT-2 Embeddings ====================
print("\n" + "="*60)
print("4. ç”ŸæˆGPT-2 Embeddings")
print("="*60)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
gpt2_model = GPT2Model.from_pretrained('gpt2').to(device)
gpt2_model.eval()

def generate_embeddings_batch(prompts, batch_size=32, desc="Processing"):
    """æ‰¹é‡ç”Ÿæˆembeddings"""
    embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(prompts), batch_size), desc=desc):
            batch_prompts = prompts[i:i+batch_size]
            inputs = tokenizer(batch_prompts, return_tensors='pt', 
                             padding=True, truncation=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            outputs = gpt2_model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, -1, :].cpu().numpy()
            embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

print("æ­£åœ¨ç”Ÿæˆè®­ç»ƒé›†embeddings...")
train_embeddings = generate_embeddings_batch(train_prompts, desc="è®­ç»ƒé›†")

print("æ­£åœ¨ç”ŸæˆéªŒè¯é›†embeddings...")
val_embeddings = generate_embeddings_batch(val_prompts, desc="éªŒè¯é›†")

print("æ­£åœ¨ç”Ÿæˆæµ‹è¯•é›†embeddings...")
test_embeddings = generate_embeddings_batch(test_prompts, desc="æµ‹è¯•é›†")

print(f"\nâœ“ Embeddingsç”Ÿæˆå®Œæˆ")
print(f"  Embeddingç»´åº¦: {train_embeddings.shape[1]}")

# ==================== 5. ä¿å­˜Embeddings ====================
print("\n" + "="*60)
print("5. ä¿å­˜Embeddingsåˆ°æœ¬åœ°")
print("="*60)

# åˆ†å—ä¿å­˜ï¼ˆæ¯1000ä¸ªæ ·æœ¬ä¸€ä¸ªæ–‡ä»¶ï¼‰
def save_embeddings_in_chunks(embeddings, save_dir, chunk_size=1000):
    num_chunks = (len(embeddings) + chunk_size - 1) // chunk_size
    for i in range(num_chunks):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, len(embeddings))
        chunk_data = embeddings[start_idx:end_idx]
        np.save(f"{save_dir}/embeddings_{i}.npy", chunk_data)
    return num_chunks

train_chunks = save_embeddings_in_chunks(train_embeddings, f"{SAVE_DIR}/train")
val_chunks = save_embeddings_in_chunks(val_embeddings, f"{SAVE_DIR}/val")
test_chunks = save_embeddings_in_chunks(test_embeddings, f"{SAVE_DIR}/test")

print(f"âœ“ è®­ç»ƒé›†: {train_chunks}ä¸ªæ–‡ä»¶ä¿å­˜è‡³ {SAVE_DIR}/train/")
print(f"âœ“ éªŒè¯é›†: {val_chunks}ä¸ªæ–‡ä»¶ä¿å­˜è‡³ {SAVE_DIR}/val/")
print(f"âœ“ æµ‹è¯•é›†: {test_chunks}ä¸ªæ–‡ä»¶ä¿å­˜è‡³ {SAVE_DIR}/test/")

# ä¿å­˜å…ƒæ•°æ®
metadata = {
    'look_back': LOOK_BACK,
    'n_future': N_FUTURE,
    'train_size': len(train_embeddings),
    'val_size': len(val_embeddings),
    'test_size': len(test_embeddings),
    'embedding_dim': train_embeddings.shape[1],
    'data_file': DATA_FILE
}

import json
with open(f"{SAVE_DIR}/metadata.json", 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"âœ“ å…ƒæ•°æ®å·²ä¿å­˜è‡³ {SAVE_DIR}/metadata.json")

print("\n" + "="*60)
print("ğŸ‰ ETDataset Embeddingsç”Ÿæˆå®Œæˆï¼")
print("="*60)
print(f"æ€»æ ·æœ¬æ•°: {len(train_embeddings) + len(val_embeddings) + len(test_embeddings)}")
print(f"å­˜å‚¨è·¯å¾„: {SAVE_DIR}/")
print(f"ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬äº†ï¼špython ETT_LSTM_LLM_offline.py")
print("="*60)
